{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MwnIjI4XXMB"
   },
   "source": [
    "# **STELLAR**: A Structured, Trustworthy, and Explainable LLM-Led Architecture for Reliable Customer Support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAy-DGlJXirS"
   },
   "source": [
    "# 1. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 22074,
     "status": "ok",
     "timestamp": 1745024741397,
     "user": {
      "displayName": "Matheus Ferracciú Scatolin",
      "userId": "06121676812765815249"
     },
     "user_tz": 180
    },
    "id": "NVDEFs37XUro",
    "outputId": "41e14a38-d850-420a-a0e5-4cd8e2caa12f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
      "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.5)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
      "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.52)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.25.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.21.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.16)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.32.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Installation of Required Libraries\n",
    "!pip install groq langchain chromadb sentence-transformers langchain-community langchain-huggingface rank_bm25 -U\n",
    "\n",
    "# Google Drive and Groq API Initialization\n",
    "from google.colab import drive, userdata\n",
    "drive.mount('/content/drive')\n",
    "from groq import Groq\n",
    "client = Groq(api_key=userdata.get('GROQ_API_KEY_3'))\n",
    "STELLAR_path = \"/content/drive/MyDrive/STELLAR\"\n",
    "\n",
    "# Essential Imports\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from math import ceil\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pytz\n",
    "import requests\n",
    "import uuid\n",
    "import logging\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Library-specific Imports\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.schema.document import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    logging as transformers_logging\n",
    ")\n",
    "\n",
    "# Set Logging Configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logging.set_verbosity_error()\n",
    "\n",
    "# Preloading Resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Model Configuration\n",
    "MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "\n",
    "class Module:\n",
    "    def __init__(self, model_name=\"llama-3.3-70b-versatile\"):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def ask_model(self, question: str, model: str=\"llama-3.3-70b-versatile\") -> str:\n",
    "        \"\"\"\n",
    "        Queries the Groq API with a question and model.\n",
    "\n",
    "        Args:\n",
    "            question (str): The input question for the model.\n",
    "            model (str): The model to query.\n",
    "\n",
    "        Returns:\n",
    "            str: The response from the model as a string or None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": question}],\n",
    "                model=model,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying Groq API: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_txt_file(self, path):\n",
    "        \"\"\"\n",
    "        Loads a text file from the specified path.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the text file.\n",
    "\n",
    "        Returns:\n",
    "            str: The contents of the text file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(path, 'r') as file:\n",
    "                return file.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {path}\")\n",
    "            return \"\"\n",
    "\n",
    "    def load_json_file(self, path:str) -> dict:\n",
    "        \"\"\"\n",
    "        Loads a JSON file from the specified path.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the JSON file.\n",
    "\n",
    "        Returns:\n",
    "            dict: The contents of the JSON file as a dictionary.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(path, 'r') as file:\n",
    "                return json.load(file)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {path}\")\n",
    "            return {}\n",
    "\n",
    "    def parse_str_to_json(self, string, required_fields):\n",
    "        \"\"\"\n",
    "        Parses a string containing a JSON-like dictionary and verifies the required fields.\n",
    "\n",
    "        Args:\n",
    "            string (str): The string to parse, expected to contain a JSON-like dictionary.\n",
    "            required_fields (list): The fields that the dictionary must contain.\n",
    "\n",
    "        Returns:\n",
    "            dict: The parsed dictionary if successful.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the parsing fails or the dictionary does not contain the required fields.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract the JSON-like content from the string\n",
    "            json_start = string.find(\"{\")\n",
    "            json_end = string.find(\"}\") + 1\n",
    "            if json_start == -1 or json_end == -1:\n",
    "                raise ValueError(\"No valid JSON content found in the string.\")\n",
    "\n",
    "            json_string = string[json_start:json_end]\n",
    "            parsed_dict = json.loads(json_string)\n",
    "\n",
    "            # Verify required fields\n",
    "            if not all(field in parsed_dict for field in required_fields):\n",
    "                raise ValueError(f\"Missing required fields: {required_fields}\")\n",
    "\n",
    "            return parsed_dict\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error parsing string to JSON: {e}\")\n",
    "\n",
    "    def get_current_date_time(self, timezone:str='America/Sao_Paulo') -> dict:\n",
    "        \"\"\"\n",
    "        Gets the current date and time in the specified timezone.\n",
    "\n",
    "        Args:\n",
    "            timezone (str): The timezone to use.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the current time, in the format:\n",
    "              {\"year\": <int>, \"month\": <int>, \"day\": <int>, \"hour\": <int>}\n",
    "\n",
    "        \"\"\"\n",
    "        timezone = pytz.timezone('America/Sao_Paulo')\n",
    "        timezone_time = datetime.now(timezone)\n",
    "        time = {\n",
    "            \"year\": timezone_time.year,\n",
    "            \"month\": timezone_time.month,\n",
    "            \"day\": timezone_time.day,\n",
    "            \"hour\": timezone_time.hour\n",
    "        }\n",
    "\n",
    "        return time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "US8rxwLrYNw7"
   },
   "source": [
    "# 2. Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDaSsXz0YPfO"
   },
   "source": [
    "## 2.1 Module 1: Initial Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Run_-KFIX3pi"
   },
   "outputs": [],
   "source": [
    "class Module1_InitialClassifier(Module):\n",
    "    def __init__(self, model_name:str=MODEL, prompt_path:str=f\"{STELLAR_path}/requirements/module_1/prompt_module1.txt\"):\n",
    "        super().__init__(model_name)\n",
    "        self.prompt_path = prompt_path\n",
    "\n",
    "\n",
    "    def classify_question(self, question: str) -> dict:\n",
    "        \"\"\"\n",
    "        Receives a question and returns the response in JSON format. Retries up to 3 times\n",
    "        if the response is not correctly formatted. If still invalid, returns a default answer.\n",
    "\n",
    "        Args:\n",
    "            question (str): The question to send to the LLM.\n",
    "            model (str): The model to query.\n",
    "\n",
    "        Returns:\n",
    "            dict: Validated response from the LLM or a default fallback answer. format:\n",
    "            {\"category\": <int>, \"answer\": <str>, \"explanation\": <str>, \"confidence\": <float>}\n",
    "        \"\"\"\n",
    "        # Get the prompt\n",
    "        with open(self.prompt_path, \"r\") as f:\n",
    "            prompt = f.read()\n",
    "\n",
    "        # Add the question to the prompt\n",
    "        full_prompt = prompt + question\n",
    "\n",
    "        default_answer = {\n",
    "            'category': 0,\n",
    "            'answer': 'Desculpe. Houve um erro com o processamento da sua dúvida. Você está sendo encaminhado para o buscador de perguntas frequentes.',\n",
    "            'explanation': 'Resposta padrão.',\n",
    "            'confidence': 1\n",
    "        }\n",
    "\n",
    "        for attempt in range(3):\n",
    "            response = self.ask_model(full_prompt, self.model_name)\n",
    "\n",
    "            if response is None:\n",
    "                print(f\"Attempt {attempt + 1}: No response from model.\")\n",
    "                continue\n",
    "\n",
    "            parsed_response = self.parse_str_to_json(response, ['category', 'answer', 'explanation', 'confidence'])\n",
    "            if type(parsed_response[\"category\"]) != int or parsed_response[\"category\"] not in [0, 1, 2, 3]:\n",
    "                print(f\"Attempt {attempt + 1}: Invalid response format.\")\n",
    "                continue\n",
    "\n",
    "            if parsed_response:\n",
    "                return parsed_response\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1}: Response parsing failed.\")\n",
    "\n",
    "        print(\"All attempts failed. Returning default answer.\")\n",
    "        return default_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2307,
     "status": "ok",
     "timestamp": 1745024743729,
     "user": {
      "displayName": "Matheus Ferracciú Scatolin",
      "userId": "06121676812765815249"
     },
     "user_tz": 180
    },
    "id": "YHs35_q1l7gy",
    "outputId": "82981df8-ee10-452c-bfd2-df569dbf9b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': 'The customer explicitly requests to speak with a human agent, indicating a need for direct assistance that may not be fully addressed through automated or FAQ channels.', 'category': 2, 'answer': \"I'm going to connect you with one of our insurance agents. Please wait for a moment.\", 'confidence': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# question = \"How do I activate my car insurance?\"  # 0\n",
    "# question = \"Where can I call to cancel my health insurance?\"  # 1\n",
    "question = \"I need the help of an insurance agent\"  # 2\n",
    "#question = \"How much is the kg of rice?\"  # 3\n",
    "\n",
    "\n",
    "module = Module1_InitialClassifier()\n",
    "print(module.classify_question(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOIX25GqZTCB"
   },
   "source": [
    "## 2.2 Module 2: RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFL3x-1JZe_C"
   },
   "outputs": [],
   "source": [
    "class Module2_RAG(Module):\n",
    "    def __init__(self, model_name=MODEL, data_path: str = f\"{STELLAR_path}/requirements/module_2/FAQs.json\",\n",
    "                 chroma_path: str = f\"{STELLAR_path}/requirements/module_2/chroma\",\n",
    "                 prompt_template_path: str = f\"{STELLAR_path}/requirements/module_2/prompt_template.txt\",\n",
    "                 prompt_rerank_path: str = f\"{STELLAR_path}/requirements/module_2/prompt_rerank.txt\",\n",
    "                 embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "                 n_retrieved_chunks: int = 10):\n",
    "        super().__init__(model_name)\n",
    "        self.data_path = data_path\n",
    "        self.chroma_path = chroma_path\n",
    "        self.prompt_template_path = prompt_template_path\n",
    "        self.prompt_rerank_path = prompt_rerank_path\n",
    "        self.embedding_model = embedding_model\n",
    "        self.n_retrieved_chunks = n_retrieved_chunks\n",
    "\n",
    "    def add_new_faq_to_chroma(self, new_faq: dict):\n",
    "        \"\"\"\n",
    "        Adds a single new FAQ to the Chroma database and to the json file with all the FAQs.\n",
    "\n",
    "        Args:\n",
    "            new_faq (dict): The new FAQ to be added to the database in this format:\n",
    "                {\"category\": <str>, \"question\": <str>, \"answer\": <str> }.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # calculate the new chunk id (based on the existing ids)\n",
    "        with open (self.data_path, \"r\") as f:\n",
    "          faqs = json.load(f)\n",
    "        max_id = max([int(faq[\"id\"]) for faq in faqs])\n",
    "        new_id = max_id + 1\n",
    "        new_faq[\"id\"] = new_id\n",
    "\n",
    "        # Format the chunk ({\"id\": <int>, \"category\": <str>, \"question\": <str>, \"answer\": <str> })\n",
    "        formated_faq = {\n",
    "            \"id\": new_id,\n",
    "            \"category\": new_faq[\"category\"],\n",
    "            \"question\": new_faq[\"question\"],\n",
    "            \"answer\": new_faq[\"answer\"]\n",
    "        }\n",
    "\n",
    "        # Write FAQ to the json file\n",
    "        faqs.append(formated_faq)\n",
    "        with open (self.data_path, \"w\") as f:\n",
    "            json.dump(faqs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        # Load the database\n",
    "        path = f\"{self.chroma_path}/vector_database\"\n",
    "        database = Chroma(\n",
    "            persist_directory=path,\n",
    "            embedding_function=self.get_embedding_function(self.embedding_model))\n",
    "\n",
    "        # Add the chunks to the database\n",
    "        documents = [formated_faq]\n",
    "        self.add_to_chroma(documents, model_name=self.embedding_model)\n",
    "\n",
    "    def create_database(self):\n",
    "        \"\"\"\n",
    "        Creates the Chroma database.\n",
    "        \"\"\"\n",
    "        # If the database already exists, return\n",
    "        path = f\"{self.chroma_path}/vector_database\"\n",
    "        if os.path.exists(path):\n",
    "          return\n",
    "        # load the items from the .json file\n",
    "        documents = self.load_documents()\n",
    "        # Save the chunks to the chroma directory\n",
    "        self.add_to_chroma(documents, model_name=self.embedding_model)\n",
    "\n",
    "    def add_to_chroma(self, chunks: list[Document], model_name: str):\n",
    "        \"\"\"\n",
    "        Adds the documents to the Chroma database.\n",
    "\n",
    "        Args:\n",
    "            chunks (list[Document]): The documents to add to the database in this format:\n",
    "                {\"id\": <int>, \"category\": <str>, \"question\": <str>, \"answer\": <str>}\n",
    "            model_name (str): The name of the embedding model to use.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Load the database\n",
    "        path = f\"{self.chroma_path}/vector_database\"\n",
    "        database = Chroma(\n",
    "            persist_directory=path,\n",
    "            embedding_function=self.get_embedding_function(model_name))\n",
    "        # Add the chunks to the database\n",
    "        existing_items = database.get(include=[]) # IDs are included by default\n",
    "        existing_ids = set(existing_items[\"ids\"])\n",
    "        print(f\"Number of existing items: {len(existing_ids)}.\")\n",
    "        # Filter out the chunks that are already in the database\n",
    "        chunks_to_add = []\n",
    "        for chunk in chunks:\n",
    "            if chunk[\"id\"] not in existing_ids:\n",
    "                chunks_to_add.append(chunk)\n",
    "        # Add the chunks to the database\n",
    "        if len(chunks_to_add) > 0:\n",
    "            print(f\"Adding {len(chunks_to_add)} items to the database.\")\n",
    "            documents_to_add = []\n",
    "            for chunk in chunks_to_add:\n",
    "                # Create a Document object for each chunk\n",
    "                document = Document(\n",
    "                    page_content=chunk[\"question\"] + \"\\n\" + chunk[\"answer\"],\n",
    "                    metadata={\n",
    "                        \"id\": chunk[\"id\"],\n",
    "                        \"category\": chunk[\"category\"],\n",
    "                        \"question\": chunk[\"question\"],\n",
    "                        \"answer\": chunk[\"answer\"]})\n",
    "                documents_to_add.append(document)\n",
    "            # Add documents to the database\n",
    "            database.add_documents(documents=documents_to_add)\n",
    "            database.persist()\n",
    "            print(\"✅ Database updated with new items.\")\n",
    "        else:\n",
    "            print(\"✅ No new items to add.\")\n",
    "\n",
    "    def load_documents(self) -> list[Document]:\n",
    "        \"\"\"Loads the documents from the data directory.\n",
    "        Documents are dictionaries with the fields:\n",
    "        id, category, question, answer.\n",
    "\n",
    "        Returns:\n",
    "            list[Document]: A list of Document objects in this format:\n",
    "            {\"id\": <int>, \"category\": <str>, \"question\": <str>, \"answer\": <str>}\n",
    "        \"\"\"\n",
    "        with open(self.data_path, \"r\") as f:\n",
    "          documents = json.load(f)\n",
    "        return documents\n",
    "\n",
    "    def get_embedding_function(self, embedding_model):\n",
    "        \"\"\"\n",
    "        Gets the embedding function for the given embedding model.\n",
    "        Args:\n",
    "            embedding_model (str): The name of the embedding model.\n",
    "        Returns:\n",
    "            HuggingFaceEmbeddings: The embedding function.\n",
    "        \"\"\"\n",
    "        model = f\"sentence-transformers/{embedding_model}\"\n",
    "        model_kwargs = {\"device\": \"cpu\"}\n",
    "        encode_kwargs = {\"normalize_embeddings\": True}\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs\n",
    "        )\n",
    "        return embeddings\n",
    "\n",
    "    def semantic_retrieval(self, question: str, n:int, model_name:str) -> list[int]:\n",
    "        \"\"\"Receives a question and retrieves n FAQ ids related to the question\n",
    "        Args:\n",
    "            question (str): The question to be answered.\n",
    "            n (int): The number of FAQs to be retrieved.\n",
    "        Return:\n",
    "            a list of the ids of the n FAQs related to the question\n",
    "        \"\"\"\n",
    "        # Prepare the database.\n",
    "        embedding_function = self.get_embedding_function(self.embedding_model)\n",
    "        path = f\"{self.chroma_path}/vector_database\"\n",
    "        database = Chroma(persist_directory=path, embedding_function=embedding_function)\n",
    "\n",
    "        # Search the database (results:List[Tuple[Document, float]]).\n",
    "        results = database.similarity_search_with_score(question, k=n)\n",
    "\n",
    "        # Return only the ids\n",
    "        for i in range(len(results)):\n",
    "            results[i] = results[i][0].metadata[\"id\"]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def BM25_retrieval(self, question: str, n: int, list_of_chunks: list[str]) -> list[int]:\n",
    "        \"\"\"\n",
    "        Retrieve top-n chunks using BM25 based on the given question.\n",
    "        Args:\n",
    "            question (str): The query question.\n",
    "            n (int): Number of chunks to retrieve.\n",
    "            list_of_chunks (List[str]): The list of chunks to search within.\n",
    "        Returns:\n",
    "            List[int]: ids of the selected chunks\n",
    "        \"\"\"\n",
    "        # Tokenize the chunks and the question\n",
    "        tokenized_chunks = [nltk.word_tokenize(chunk) for chunk in list_of_chunks]\n",
    "        tokenized_question = nltk.word_tokenize(question)\n",
    "\n",
    "        # Initialize the BM25 model\n",
    "        bm25 = BM25Okapi(tokenized_chunks)\n",
    "\n",
    "        # Score the chunks based on the question\n",
    "        scores = bm25.get_scores(tokenized_question)\n",
    "\n",
    "        # Get the indices of the top-n chunks based on scores\n",
    "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:n]\n",
    "\n",
    "        return top_indices\n",
    "\n",
    "\n",
    "    def hybrid_search(self, question: str, n: int, embedding_model: str) -> list[int]:\n",
    "      \"\"\"Receives a question and retrieves n FAQs related to the question\n",
    "      Return: a list of the ids of the n FAQs related to the question\n",
    "      \"\"\"\n",
    "      # 70% Semantic search (round up)\n",
    "      n_semantic = ceil(0.7 * n)\n",
    "      semantic_ids = self.semantic_retrieval(question, n_semantic, embedding_model)\n",
    "\n",
    "\n",
    "      # 30% BM25 search\n",
    "      n_bm25 = n - n_semantic\n",
    "\n",
    "      # Search for n chunks so that we can choose the 30% most similar that are not in semantic_ids\n",
    "      with open (self.data_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "      # Use the \"question only\" approach by default\n",
    "      chunks = [data[i][\"question\"] for i in range(len(data))]\n",
    "      bm25_ids = self.BM25_retrieval(question, n, chunks)\n",
    "      # Choose the 30% most similar that are not in semantic_ids\n",
    "      bm25_ids = [i for i in bm25_ids if i not in semantic_ids]\n",
    "      bm25_ids = bm25_ids[:n_bm25]\n",
    "\n",
    "      hybrid_ids = semantic_ids + bm25_ids\n",
    "      return hybrid_ids\n",
    "\n",
    "    def error_handler(self, faq_ids: list[int], reranked_ids: list[int]) -> list[int]:\n",
    "        \"\"\"\n",
    "        Adjusts a reranked list of IDs to match the FAQ IDs list in length and content.\n",
    "\n",
    "        Args:\n",
    "            faq_ids (List[int]): The original list of FAQ IDs.\n",
    "            reranked_ids (List[int]): The reranked list of IDs.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: A list with the correct number of unique IDs matching FAQ IDs.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If reranked_ids is not a list.\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate input type\n",
    "        if not isinstance(reranked_ids, list):\n",
    "            raise Exception(\"The reranked_ids variable is not a list\")\n",
    "\n",
    "        # Ensure reranked_ids contains only unique integers\n",
    "        reranked_ids = [rr_id for rr_id in reranked_ids if isinstance(rr_id, int)]\n",
    "        reranked_ids = list(dict.fromkeys(reranked_ids))  # Remove duplicates\n",
    "\n",
    "        # Case 1: If sizes match, return as-is\n",
    "        if len(reranked_ids) == len(faq_ids):\n",
    "            return reranked_ids\n",
    "\n",
    "        # Case 2: If reranked_ids is smaller, fill missing IDs from faq_ids\n",
    "        if len(reranked_ids) < len(faq_ids):\n",
    "            missing_ids = [faq_id for faq_id in faq_ids if faq_id not in reranked_ids]\n",
    "            return reranked_ids + missing_ids[:len(faq_ids) - len(reranked_ids)]\n",
    "\n",
    "        # Case 3: If reranked_ids is larger, filter out extras and recurse\n",
    "        reranked_ids = [rr_id for rr_id in reranked_ids if rr_id in faq_ids]\n",
    "        return self.error_handler(faq_ids, reranked_ids)\n",
    "\n",
    "\n",
    "    def groq_LLM_reranking(self, query: str, faq_ids: list[int], model: str, answers:bool = True) -> list[str]:\n",
    "        \"\"\"\n",
    "        Reranks a list of FAQs based on their relevance to the query using Groq API.\n",
    "\n",
    "        Args:\n",
    "            query (str): The question from the customer.\n",
    "            faq_ids (List[int]): A list of the ids of the FAQs returned after a dense retrieval.\n",
    "            model (str): The model to use for reranking.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The same FAQ ids reranked by relevance.\n",
    "        \"\"\"\n",
    "        def load_faqs(self, faq_ids: list[int]) -> list[tuple]:\n",
    "            \"\"\"Loads FAQs matching the given IDs from the data file.\n",
    "            Retuns:\n",
    "                List[Tuple[str, str, int]]\n",
    "            \"\"\"\n",
    "            with open(self.data_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            return [(faq[\"question\"], faq[\"answer\"], faq[\"id\"]) for faq in data if faq[\"id\"] in faq_ids]\n",
    "        def build_prompt(self, query: str, faqs_with_ids: list[tuple], answers:bool = False) -> str:\n",
    "            \"\"\"Constructs the prompt for the model.\"\"\"\n",
    "            with open(self.prompt_rerank_path, 'r') as file:\n",
    "                prompt = file.read()\n",
    "            if not answers:\n",
    "              related_questions = \"\\n\".join(f\"{faq_id}: {content}\" for content, answer, faq_id in faqs_with_ids)\n",
    "              return f\"{prompt}\\nDúvida do cliente:\\n{query}\\nPerguntas relacionadas:\\n{related_questions}\\n\\nSaída:\"\n",
    "            else:\n",
    "              related_questions = \"\\n\".join(f\"{faq_id}: {content}\\n{answer}\" for content, answer, faq_id in faqs_with_ids)\n",
    "              return f\"{prompt}\\nDúvida do cliente:\\n{query}\\nPerguntas relacionadas:\\n{related_questions}\\n\\nSaída:\"\n",
    "        def parse_model_response(self, response: str) -> list[int]:\n",
    "            \"\"\"Parses the model's response into a list of IDs.\"\"\"\n",
    "            start = response.find(\"[\")\n",
    "            end = response.find(\"]\") + 1\n",
    "            return json.loads(response[start:end])\n",
    "        def call_model_with_retries(self, prompt: str, max_retries: int = 3) -> list[int]:\n",
    "            \"\"\"Attempts to call the model with retries in case of failure.\"\"\"\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    response = self.ask_model(prompt, model)\n",
    "                    reranked_ids = parse_model_response(self, response)\n",
    "                    reranked_ids = self.error_handler(faq_ids, reranked_ids)\n",
    "\n",
    "                    if isinstance(reranked_ids, list) and len(reranked_ids) == len(faq_ids):\n",
    "                        return reranked_ids\n",
    "                    else:\n",
    "                        raise ValueError(\"Invalid response format or length mismatch.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "\n",
    "            # Fall back to the original FAQ order if all retries fail\n",
    "            print(\"Model inference failed after all retries.\")\n",
    "            return faq_ids\n",
    "        # Main process\n",
    "        with open(self.prompt_rerank_path, 'r') as file:\n",
    "            prompt = file.read()\n",
    "        try:\n",
    "            faqs_with_ids = load_faqs(self, faq_ids)\n",
    "            prompt = build_prompt(self, query=query, faqs_with_ids=faqs_with_ids)\n",
    "            return call_model_with_retries(self, prompt)\n",
    "        except Exception as e:\n",
    "            print(\"Unexpected error:\", e)\n",
    "            return faq_ids\n",
    "\n",
    "    def query_model(self, question: str, FAQ_ids, n_ids):\n",
    "        \"\"\"\n",
    "        Queries the LLM with the retrieved FAQS to generate the answer to the client.\n",
    "        Args:\n",
    "            question (str): The question from the customer.\n",
    "            FAQ_ids (List[int]): A list of the ids of the FAQs returned after a hybrid retrieval.\n",
    "            n_ids (int): The number of FAQs to be retrieved.\n",
    "\n",
    "        Returns:\n",
    "            str: The answer to the question.\n",
    "        \"\"\"\n",
    "        # Use the n_ids chunks with the highest similarity\n",
    "        FAQ_ids = FAQ_ids[:n_ids]\n",
    "\n",
    "        # create the context\n",
    "        with open (self.data_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        chunks = []\n",
    "        for id in FAQ_ids:\n",
    "          for faq in data:\n",
    "            if faq[\"id\"] == id:\n",
    "              chunks.append(faq[\"question\"]+\"\\n\"+faq[\"answer\"])\n",
    "              break\n",
    "\n",
    "        context_text = \"\\n---\\n\".join([chunk for chunk in chunks])\n",
    "        # Apply the prompt template to the query\n",
    "        with open(self.prompt_template_path, 'r') as file:\n",
    "            PROMPT_TEMPLATE = file.read()\n",
    "        prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "        prompt = prompt_template.format(context=context_text, question=question)\n",
    "\n",
    "        model_answer = self.ask_model(prompt, self.model_name)\n",
    "\n",
    "        # Format the response\n",
    "        sources = FAQ_ids\n",
    "        formatted_response = f\"\\033[34mAnswer: {model_answer}\\033[0m\\n\\033[32mSources (FAQ ids): {sources}\\033[0m\"\n",
    "        return formatted_response\n",
    "\n",
    "    # Main function to handle the entire RAG process\n",
    "    def process_question(self, question: str, n_ids: int = 5)->str:\n",
    "        \"\"\"\n",
    "        Retrieves FAQs from the database, reranks these FAQs\n",
    "        and answers the question based on them.\n",
    "\n",
    "        Args:\n",
    "            question (str): The question to be answered.\n",
    "            n_ids (int): The number of FAQs to be retrieved.\n",
    "\n",
    "        Returns:\n",
    "            str: The answer to the question.\n",
    "        \"\"\"\n",
    "        # Step 1: Retrieve IDs from the database\n",
    "        retrieved_ids = self.hybrid_search(question, self.n_retrieved_chunks, self.embedding_model)\n",
    "\n",
    "        # Step 2: Re-rank the retrieved IDs\n",
    "        reranked_ids = self.groq_LLM_reranking(\n",
    "            question,\n",
    "            retrieved_ids,\n",
    "            self.model_name,\n",
    "            answers=True\n",
    "        )\n",
    "\n",
    "        # Step 3: Query the model with the top `n_ids` chunks\n",
    "        formatted_response = self.query_model(question, reranked_ids, n_ids)\n",
    "        return formatted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "991a7bf9a7ce4c2a9e95bc4f6f28709c",
      "0c9738c2789a4acc9f8e152dc061190d",
      "71569728aa984c07871f87eca1234a2d",
      "b173bf8416674ae9976b92835d311eb4",
      "1a765679c2144488aef6c0710b1fd791",
      "90b08533d4d24232986069beea453e10",
      "aac90e0c1a9b4a9cb5894b0f0d98a00d",
      "b0a228d0691b4210b462695902370c7d",
      "94eaedcf315a4989acf1b7d138faca59",
      "a0a1361c75314cf7b062312c68c26def",
      "918362d074274842a0a5324a9592e4d6",
      "15926d63fbba406c806c7c5514222082",
      "edbc9042319e4e78be6f11eba20760cb",
      "f714f0fa611641aea98ec1c4839ef08d",
      "98ca8c588c124ac6a967e188cc5fd5cd",
      "9f153a7e41ef47d5b246b100ca661a46",
      "012208a093d342949ac6ab177ceff14f",
      "322109b00ffb45e0a619167dbce3fb3e",
      "77807810eb744a83937467ddfb8a0327",
      "ab23862cd3ca491bab29c7cded46f026",
      "baae696b09e0484b8777cec8c5f73a18",
      "0b7f165372b141c685595b6fc142ac50",
      "444bff43c6ed48c795960d9a7e94162b",
      "686ca45b18914039af5235534cd38ba4",
      "fabbc34f673d4e5eaeef6ad724d3a83d",
      "3028cfdfda394bb8a2e4192840756b98",
      "6147223a294f407bbe46667adb36ae99",
      "8a4a874d29104c3bb78e63d0dcf3e2cf",
      "66f9872c586d491eb5cfbd45391e9ab3",
      "0e3681c4c79a4df7bcb17b21b56939ef",
      "98b3ccbbdac94a42bc91008b9c92d1b2",
      "149e2e3729de4499ab31bc0e99a28c22",
      "56a0fff66f044e7d980e469f5ee91350",
      "d216e7748ef745d08d116eb8ee24dda9",
      "ff6e485cf4174c9c81c36e1a17173b17",
      "795fdf7605af4bc1b0eccd9f6b5e4db1",
      "fbde4d90e44f44168928e873c300fd38",
      "4f3683cf16284fdf9271461a93f8b3fb",
      "dbfb0eb7074f43cba02f787f4e8f278e",
      "d1ecd1ff66bb4941b730aa4a26b9c675",
      "0edad29d6be94eee9fd418109092900b",
      "274d256988a740c8b0065a2b42029d49",
      "cdbf991c2b1945f59ea57402fc86af2c",
      "1afdcd0534fc42d5a7309da3e9be2884",
      "971544a6dad347cda6920284c6114478",
      "19e6179a5b9247d08ea201d632db9430",
      "28cf5421a140427394ec22f147e96cdf",
      "a7f17c8d30094f0e8b668f7296ea0cfe",
      "260a79ee0bb8468c96dd65b44b12da6b",
      "be869fd17b5343288064b72c35a0fef2",
      "d6581bb8da6e41caa862daa224a38de6",
      "6da2bd6294464a1a873a2a6a1514ac0c",
      "31beb895d71e40d68c520bd7f95ef5a5",
      "5492670fcdfc4b67a807eb26bb545298",
      "9a55b5f9bbc948d3ad4ab3f3eb8adb0e",
      "4960771f2d18469da4b5bcd71662d92b",
      "c4a0f99ecd3447d49857ba45d71a154f",
      "ea933ad396c84225990ca590d2acd5cb",
      "f8a5b98b2e2e489fa9e00b7a127466bf",
      "efd2cc293ad640e79ef4effe36fb2e04",
      "6aeaee16e7d748babe3d9231180dcd56",
      "77f888a18a9148c98c6134b8a062a231",
      "ccfe7df61b4248169642085c8a1392c7",
      "697dfd6d091d4c9cb5931bfae9c20496",
      "10ed79571b5c492f9e45742a4073ba7a",
      "fae3e7f7d12c4887bb7cd8e81b64d4a9",
      "65f84b70818d40779f0a0a5f74555b18",
      "00d1af459d0741798ab9c35064427346",
      "6f23868567ed4bdc9203a170604d3c21",
      "6e937bb6b0664f85b545fb0469604d83",
      "94c0eab2ec924558be649d32bb135120",
      "5f061417ed9d46a28fd714cfa276ab96",
      "7bb6d7aff09a46f4a221b0419abcc59b",
      "de61ada01d804e459923d3f7eee280f9",
      "41cc987e3fa645cca034a95904c63337",
      "60f7cf523fa14035b1aaf2ce80417454",
      "38d61de834ff4c46966fada32964c358",
      "b7bf4e9fbb2647a59cc2b8e9d9905252",
      "f9970c8e6e614a209a214448009e91f3",
      "8236fedc486741e38cf4c3cfeb142fca",
      "c9f545cc0cbf435d920b664549f88929",
      "0ffd8c4f77d34e9383eb8b12954296f9",
      "f9c48e1db19e41c2a068f87b14c4d016",
      "1b35230a85254afa9329921f8eeb9eaa",
      "44f3a3893d0e40faa8428326fa67b7e7",
      "a97855440ef44849aed5bd66f33ca3d6",
      "0479886107b1467f9453028c4cc80593",
      "8ee00cd5b7c74c039fc5264257f7d194",
      "030734d4b6ec42e4a824e49422964cb0",
      "ec77abbe29f345599faf28a362444572",
      "7e2bb92e54824a9cafd0cb55a097f985",
      "3d6a0cc72f8b44eeb8b46970fb72661a",
      "9269564a993c4b76b6a44ca84b19cc31",
      "f967a3c8d4f845ba9f8003d2745e56f6",
      "169de2230d9741fbb80c799f16e5d7b9",
      "f93f279584cf4e71aa19711615e7036b",
      "d9164c6a88aa4325a72f939d10c7bc07",
      "b60d7cfa1d8946ef912d76fca31b129e",
      "826967583fd74c28bbbe4db7ab304503",
      "d34a5abbcf854a7d95db223cb3c39125",
      "ed068f6c02734499ae165f87cf62b72d",
      "3596c7b7effa4812b785635c85a19d01",
      "d5b7459713864e88bd9025a1d868494e",
      "ae19f7509eaf4bde96d61dca6af0a83e",
      "0b6376c5827b4720a261807948be7c96",
      "0125748bdd084ae6850cdadb2d402dcd",
      "8c4f313a367f49319aff33980ec5a3fd",
      "9d06f95b30c649a4987dcd78e4870970",
      "ad426544617f485a92b43e6aaeff76a8",
      "b49c162e7b0a4b5e920e22cd4444f8f2",
      "585ec66e77bd410d823ec7c1db21d80e",
      "d6ba10cbfece46f99f3010aca08fa653",
      "1ce549eb44784c77b3159fc405839fd1",
      "f50212e347eb49bf87379cf9bfb4698d",
      "3021f1346eff48b3be7a5a6b34d0f828",
      "4d60cdf1c3ed41bbbd9d975ef1d57243",
      "c33c81d395b64998a68456bc8059336d",
      "7b3b1993acab4136a77223d753552790",
      "be34d820fc31416db02f83e42dc600d8",
      "0e8e236e863845cfa5f211c02cdba8f6",
      "f8ded8d337af43b4beb45745e3e3ba92"
     ]
    },
    "executionInfo": {
     "elapsed": 59632,
     "status": "ok",
     "timestamp": 1745024803419,
     "user": {
      "displayName": "Matheus Ferracciú Scatolin",
      "userId": "06121676812765815249"
     },
     "user_tz": 180
    },
    "id": "8V6p_ZuRikGr",
    "outputId": "eb9091f2-b0cd-4616-d075-43cd78e4f1dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991a7bf9a7ce4c2a9e95bc4f6f28709c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15926d63fbba406c806c7c5514222082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444bff43c6ed48c795960d9a7e94162b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d216e7748ef745d08d116eb8ee24dda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971544a6dad347cda6920284c6114478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4960771f2d18469da4b5bcd71662d92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f84b70818d40779f0a0a5f74555b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bf4e9fbb2647a59cc2b8e9d9905252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030734d4b6ec42e4a824e49422964cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34a5abbcf854a7d95db223cb3c39125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585ec66e77bd410d823ec7c1db21d80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-e2eb5a36f297>:157: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  database = Chroma(persist_directory=path, embedding_function=embedding_function)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAnswer: The benefits of the dental insurance include coverage for various procedures such as extraction, root canal, surgeries, radiology, dentistry (restorations), periodontics (gum treatment), endodontics (root canals), surgeries, pediatric dentistry (dentistry for children), and prostheses. It also guarantees access to basic dental treatments like cleaning and treatment of cavities. Additionally, it has a wide national coverage with over 27,000 accredited dentists. However, it's worth noting that implants are not covered in individual plans.\u001b[0m\n",
      "\u001b[32mSources (FAQ ids): [42, 37, 38, 41, 40]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "module = Module2_RAG()\n",
    "module.create_database()\n",
    "\n",
    "question = \"What are the benefits of the dental insurance?\"\n",
    "print(module.process_question(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytSguSoejHRN"
   },
   "source": [
    "## 2.3 Module 3: Contact Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_5ahNYqjxCz"
   },
   "outputs": [],
   "source": [
    "class Module3_ContactInfo(Module):\n",
    "    def __init__(self, model_name=MODEL, contact_info_path:str=f\"{STELLAR_path}/requirements/module_3/contact_info.txt\",\n",
    "                 prompt_path:str=f\"{STELLAR_path}/requirements/module_3/initial_prompt.txt\",\n",
    "                 followup_prompt_path:str=f\"{STELLAR_path}/requirements/module_3/followup_prompt.txt\"):\n",
    "        super().__init__(model_name)\n",
    "        self.contact_info = self.load_txt_file(contact_info_path)\n",
    "        self.prompt = self.load_txt_file(prompt_path)\n",
    "        self.followup_prompt = self.load_txt_file(followup_prompt_path)\n",
    "        self.chat_history = \"\"\n",
    "\n",
    "    def ask_module(self, query:str):\n",
    "        \"\"\"\n",
    "        Interacts with a user to answer questions based on a predefined context and a language model.\n",
    "\n",
    "        This function prompts the user for a query, processes it using a predefined context template,\n",
    "        and queries a language model for an answer. It handles follow-up questions if the model\n",
    "        indicates ambiguity or the need for clarification.\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "        Returns:\n",
    "            str: The final answer provided by the language model, or an appropriate message\n",
    "                if the query cannot be answered.\n",
    "            str: The updated chat history.\n",
    "        \"\"\"\n",
    "\n",
    "        def process_follow_up(query, follow_up_question):\n",
    "            \"\"\"\n",
    "            Handles the process of asking a follow-up question and querying the model again.\n",
    "            The model is not informed that it can start the answer with \"-1\". Therefore,\n",
    "            only 1 follow-up question is asked.\n",
    "\n",
    "            Args:\n",
    "                query (str): The original user query.\n",
    "                follow_up_question (str): The follow-up question suggested by the model.\n",
    "\n",
    "            Returns:\n",
    "                str: The model's final response to the query.\n",
    "                str: The updated chat history.\n",
    "            \"\"\"\n",
    "            print(f\"Additional question: {follow_up_question}\")\n",
    "            follow_up_answer = input(\"Please, provide more details: \")\n",
    "\n",
    "            self.chat_history += f\"\\n\\033[32mUser\\033[0m: {follow_up_answer}\\n\"\n",
    "\n",
    "            full_query = query + follow_up_question + follow_up_answer\n",
    "            follow_up_prompt = self.followup_prompt.format(context=self.contact_info, question=full_query)\n",
    "            follow_up_response = self.ask_model(follow_up_prompt, MODEL)\n",
    "\n",
    "            return handle_response(follow_up_response, full_query)\n",
    "\n",
    "        def handle_response(response, query):\n",
    "            \"\"\"\n",
    "            Processes the model's response to determine the next steps.\n",
    "\n",
    "            Args:\n",
    "                response (str): The model's response.\n",
    "                query (str): The query associated with the response.\n",
    "\n",
    "            Returns:\n",
    "                str: The final answer or an appropriate error message.\n",
    "                str: The updated chat history.\n",
    "            \"\"\"\n",
    "            if response.startswith(\"-1\"):\n",
    "                follow_up_question = response[3:]\n",
    "                self.chat_history += f\"\\n\\033[34mModule 3\\033[0m: {follow_up_question}\\n\"\n",
    "                return process_follow_up(query, follow_up_question)\n",
    "\n",
    "            if response.startswith(\"-2\"):\n",
    "                return \"Unfortunately, there is no answer to your question in our contact information.\", \"\"\n",
    "\n",
    "\n",
    "            return response, self.chat_history\n",
    "\n",
    "        # Main execution\n",
    "        prompt = self.prompt.format(context=self.contact_info, question=query)\n",
    "        model_answer = self.ask_model(prompt, MODEL)\n",
    "\n",
    "        return handle_response(model_answer, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1715,
     "status": "ok",
     "timestamp": 1745024805163,
     "user": {
      "displayName": "Matheus Ferracciú Scatolin",
      "userId": "06121676812765815249"
     },
     "user_tz": 180
    },
    "id": "oT8N2gkloB2Z",
    "outputId": "0938f26d-b075-4a32-bee8-6f9af0e53450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make a complaint about your car insurance, you can call the SAC (Customer Service) at 0800 727 9966, which is available 24 hours, 7 days a week. This service is free of charge. \n",
      "\n",
      "Alternatively, if you have a specific issue related to your auto insurance, you can also contact the Auto Assistance Activations at 4004 2757 (Capitals and Metropolitan Regions) or 0800 701 2757 (Other Regions), which is available 24 hours, 7 days a week.\n"
     ]
    }
   ],
   "source": [
    "# query = \"What is the number of my insurance?\"\n",
    "# query = \"How much is the kg of rice?\"\n",
    "query = \"Where should I call to make a complaint about my car insurance?\"\n",
    "\n",
    "module = Module3_ContactInfo()\n",
    "answer, chat_history = module.ask_module(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMEf2c9iogFN"
   },
   "source": [
    "## 2.4 Module 4: Human Escalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNeJMO7ootuK"
   },
   "outputs": [],
   "source": [
    "category_mapping = {\n",
    "  \"Gestão de Apólices\": \"Policy Management\",\n",
    "  \"Sinistros\":\"Claims\",\n",
    "  \"Pagamentos\": \"Payments\",\n",
    "  \"Perguntas Gerais\": \"General Questions\",\n",
    "  \"Problemas Técnicos\": \"Technical Problems\",\n",
    "  \"Escalações para Suporte Humano\": \"Human Support Escalations\",\n",
    "  \"Perguntas Regulatórias ou de Conformidade\": \"Regulatory or Compliance Questions\"\n",
    "}\n",
    "\n",
    "class Module4_HumanEscalation(Module):\n",
    "    def __init__(self, model_name=MODEL, insurance_weights_path:str=f\"{STELLAR_path}/requirements/module_4/insurance_weights.json\",\n",
    "                 category_weights_path:str=f\"{STELLAR_path}/requirements/module_4/category_weights.json\",\n",
    "                 name_and_ins_type_prompt_path:str=f\"{STELLAR_path}/requirements/module_4/name_and_ins_type_prompt.txt\",\n",
    "                 sum_and_cat_prompt_path:str=f\"{STELLAR_path}/requirements/module_4/sum_and_cat_prompt.txt\",\n",
    "                 recommended_message_prompt_path:str=f\"{STELLAR_path}/requirements/module_4/recommended_message_prompt.txt\",\n",
    "                 waiting_list_path:str=f\"{STELLAR_path}/outputs/module_4/waiting_list.json\",\n",
    "                 human_agents_path:str=f\"{STELLAR_path}/human_agents/human_agents.json\"):\n",
    "        super().__init__(model_name)\n",
    "        self.insurance_weights = self.load_json_file(insurance_weights_path)\n",
    "        self.category_weights = self.load_json_file(category_weights_path)\n",
    "        self.name_and_ins_type_prompt = self.load_txt_file(name_and_ins_type_prompt_path)\n",
    "        self.sum_and_cat_prompt = self.load_txt_file(sum_and_cat_prompt_path)\n",
    "        self.recommended_message_prompt = self.load_txt_file(recommended_message_prompt_path)\n",
    "        self.waiting_list_path = waiting_list_path\n",
    "        self.human_agents_path = human_agents_path\n",
    "\n",
    "    def add_human_agent(self, human_agent):\n",
    "        \"\"\"\n",
    "        Adds a new human agent to the list of human agents.\n",
    "        Args:\n",
    "            human_agent (Human_agent): A dictionary containing the details of the new human agent.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.human_agents_path, \"r\") as f:\n",
    "                human_agents = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            human_agents = []\n",
    "\n",
    "        new_agent = {\n",
    "              \"id\": (len(human_agents) + 1),\n",
    "              \"status\": human_agent.status,\n",
    "              \"human_attendant_name\": human_agent.human_attendant_name,\n",
    "              \"insurance_type\": human_agent.insurance_type,\n",
    "              \"query_category\": human_agent.query_category\n",
    "        }\n",
    "\n",
    "\n",
    "        human_agents.append(new_agent)\n",
    "\n",
    "        with open(self.human_agents_path, \"w\") as f:\n",
    "            json.dump(human_agents, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    def free_human_agent(self, id):\n",
    "        \"\"\"\n",
    "        Frees an agent with the given id.\n",
    "        Args:\n",
    "            id (int): The id of the agent to be freed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.human_agents_path, \"r\") as f:\n",
    "                human_agents = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return\n",
    "\n",
    "        for agent in human_agents:\n",
    "            if agent[\"id\"] == id:\n",
    "                agent[\"status\"] = \"Available\"\n",
    "                with open(self.human_agents_path, \"w\") as f:\n",
    "                    json.dump(human_agents, f, indent=2, ensure_ascii=False)\n",
    "                return\n",
    "\n",
    "    def find_available_human_agent(self)->dict:\n",
    "        \"\"\"\n",
    "        Finds a human agent for the customer.\n",
    "        Returns:\n",
    "            dict: A dictionary containing the details of the human agent in this format:\n",
    "            {\"id\": <int>, \"status\": <str>, \"human_attendant_name\": <str>, \"insurance_type\": <int>, \"query_category\": <str>}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.human_agents_path, \"r\") as f:\n",
    "                human_agents = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return None\n",
    "\n",
    "        # Search an agent with the same query category and insurance type\n",
    "        for agent in human_agents:\n",
    "            if agent[\"status\"] == \"Available\" and agent[\"insurance_type\"] == self.insurance_type and agent[\"query_category\"] == self.query_category:\n",
    "                agent[\"status\"] = \"Busy\"\n",
    "                with open(self.human_agents_path, \"w\") as f:\n",
    "                    json.dump(human_agents, f, indent=2, ensure_ascii=False)\n",
    "                return agent\n",
    "\n",
    "        # Search for an agent with the same insurance type\n",
    "        for agent in human_agents:\n",
    "            if agent[\"status\"] == \"Available\" and agent[\"insurance_type\"] == self.insurance_type:\n",
    "                agent[\"status\"] = \"Busy\"\n",
    "                with open(self.human_agents_path, \"w\") as f:\n",
    "                    json.dump(human_agents, f, indent=2, ensure_ascii=False)\n",
    "                return agent\n",
    "        # Search for any available agents\n",
    "        for agent in human_agents:\n",
    "            if agent[\"status\"] == \"Available\":\n",
    "                agent[\"status\"] = \"Busy\"\n",
    "                with open(self.human_agents_path, \"w\") as f:\n",
    "                    json.dump(human_agents, f, indent=2, ensure_ascii=False)\n",
    "                return agent\n",
    "\n",
    "        # if no human agent is available, return None\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    def model_answer_name_and_ins_type(self, chat_history, model, max_retries=3):\n",
    "        \"\"\"\n",
    "          Processes a chat_history through the model and returns the response as a dictionary.\n",
    "          Retries once if the response is not valid JSON, and falls back to parsing_error_handler.\n",
    "          Args:\n",
    "              chat_history (str): The input chat_history.\n",
    "              model (str): The model to query.\n",
    "\n",
    "          Returns:\n",
    "              dict: The response from the model as a dictionary in the format\n",
    "              {\"name\": <str>, \"insurance_type\": <int>}.\n",
    "        \"\"\"\n",
    "        attempt = 0\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                prompt = self.name_and_ins_type_prompt + chat_history\n",
    "                answer = self.ask_model(prompt, model)\n",
    "\n",
    "                if not answer:\n",
    "                    raise ValueError(\"Answer is None\")\n",
    "\n",
    "                return self.parse_str_to_json(answer, [\"name\", \"insurance_type\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed in model_answer_name_and_ins_type: {e}\")\n",
    "                attempt += 1\n",
    "\n",
    "        # if parsing failed, return the default name and ins_type\n",
    "        output = {\n",
    "            \"name\": \"\",\n",
    "            \"insurance_type\": 0\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    def model_answer_summary_and_category(self, chat_history, model, max_retries=3):\n",
    "        \"\"\"\n",
    "        Processes a chat_history through the model and returns the response as a dictionary.\n",
    "        Retries once if the response is not valid JSON, and falls back to parsing_error_handler.\n",
    "        Args:\n",
    "            chat_history (str): The input chat_history.\n",
    "            model (str): The model to query.\n",
    "\n",
    "        Returns:\n",
    "            dict: The response from the model as a dictionary in the format\n",
    "            {\"summary\": <str>, \"category\": <str>, \"subcategory\": <str>}.\n",
    "        \"\"\"\n",
    "        attempt = 0\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                prompt = self.sum_and_cat_prompt + chat_history\n",
    "                answer = self.ask_model(prompt, model)\n",
    "\n",
    "                if not answer:\n",
    "                    raise ValueError(\"Answer is None\")\n",
    "\n",
    "                return self.parse_str_to_json(answer, [\"summary\", \"category\", \"subcategory\"])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed in model_answer_summary_and_category: {e}\")\n",
    "                attempt += 1\n",
    "\n",
    "        # if parsing failed, return empty summary, and default category and subcategory\n",
    "        output = {\n",
    "            \"summary\": \"\",\n",
    "            \"category\": \"General Questions\",\n",
    "            \"subcategory\": \"Other\"\n",
    "        }\n",
    "        return output\n",
    "\n",
    "\n",
    "    def calculate_sentiment_points(self, sentiment: dict[str, float]) -> int:\n",
    "        \"\"\"\n",
    "        Calculates sentiment points based on the sentiment distribution.\n",
    "        Formula:\n",
    "            Sentiment Factor = (negative * 2.0) + (neutral * 1.0) + (positive * 0.5)\n",
    "            Sentiment Points = min(50, max(0, Sentiment Factor * 25))\n",
    "        Args:\n",
    "            sentiment (Dict[str, float]): Dictionary with keys 'positive', 'neutral', 'negative' and their corresponding values.\n",
    "        Returns:\n",
    "            int: Sentiment points in the range [0, 50].\n",
    "        \"\"\"\n",
    "        sentiment_factor = (sentiment[\"negative\"] * 2.0) + (sentiment[\"neutral\"] * 1.0) + (sentiment[\"positive\"] * 0.5)\n",
    "        sentiment_points = min(50, max(0, int(sentiment_factor * 25)))\n",
    "        return sentiment_points\n",
    "\n",
    "    def calculate_category_points(self, category: str, subcategory: str, weights: dict[str, dict[str, float]]) -> int:\n",
    "        \"\"\"\n",
    "        Calculates category points based on predefined weights for category and subcategory.\n",
    "        Formula:\n",
    "            Category Points = Weight * 30\n",
    "        Args:\n",
    "            category (str): The category of the query.\n",
    "            subcategory (str): The subcategory of the query.\n",
    "            weights (Dict[str, Dict[str, float]]): Dictionary mapping categories and subcategories to their respective weights.\n",
    "        Returns:\n",
    "            int: Category points in the range [0, 30].\n",
    "        \"\"\"\n",
    "        weight = weights.get(category, {}).get(subcategory, 0.0)\n",
    "        category_points = min(30, max(0, int(weight * 30)))\n",
    "        return category_points\n",
    "\n",
    "    def calculate_insurance_points(self, insurance_type: int, weights: dict[int, float]) -> int:\n",
    "        \"\"\"\n",
    "        Calculates insurance points based on predefined weights for insurance types.\n",
    "        Formula:\n",
    "            Insurance Points = Weight * 20\n",
    "        Args:\n",
    "            insurance_type (int): The insurance type ID.\n",
    "            weights (Dict[int, float]): Dictionary mapping insurance types to their respective weights.\n",
    "        Returns:\n",
    "            int: Insurance points in the range [0, 20].\n",
    "        \"\"\"\n",
    "        weight = weights.get(insurance_type, 0.0)\n",
    "        insurance_points = min(20, max(0, int(weight * 20)))\n",
    "        return insurance_points\n",
    "\n",
    "    def calculate_urgency(self, sentiment: dict[str, float], category: str, subcategory: str, insurance_type: int) -> int:\n",
    "        \"\"\"\n",
    "        Calculates the total urgency score based on sentiment, category/subcategory, and insurance type.\n",
    "        Formula:\n",
    "            Urgency Score = Sentiment Points + Category Points + Insurance Points\n",
    "        Args:\n",
    "            sentiment (Dict[str, float]): Sentiment analysis scores.\n",
    "            category (str): Query category.\n",
    "            subcategory (str): Query subcategory.\n",
    "            insurance_type (int): Type of insurance.\n",
    "        Returns:\n",
    "            int: Total urgency score in the range [0, 100].\n",
    "        \"\"\"\n",
    "        sentiment_points = self.calculate_sentiment_points(sentiment)\n",
    "        category_points = self.calculate_category_points(category, subcategory, self.category_weights)\n",
    "        insurance_points = self.calculate_insurance_points(insurance_type, self.insurance_weights)\n",
    "\n",
    "        urgency_score = sentiment_points + category_points + insurance_points\n",
    "        return urgency_score\n",
    "\n",
    "    def recommended_message(self, chat_history: str, human_attendant_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a recommended introductory message for the human attendant to send to the customer.\n",
    "\n",
    "        Args:\n",
    "            chat_history (str): The history of the conversation with the customer.\n",
    "            human_attendant_name (str): The name of the human attendant.\n",
    "\n",
    "        Returns:\n",
    "            str: The recommended message for the human attendant to send to the customer.\n",
    "        \"\"\"\n",
    "        # Format the prompt with the input data\n",
    "        question = self.recommended_message_prompt.format(\n",
    "            human_attendant_name=human_attendant_name,\n",
    "            chat_history=chat_history,\n",
    "        )\n",
    "\n",
    "        # Ask the model to generate the response\n",
    "        response = self.ask_model(question, self.model_name)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def add_to_waiting_list(self, issue_data:dict):\n",
    "        \"\"\"\n",
    "        Adds the customer to the waiting list.\n",
    "        Args:\n",
    "            issue_data (dict): The data of the customer in the following format:\n",
    "            {\"sentiment\": <dict>, \"chat_history\": <str>, \"human_attendant_name\": <str>, \"model\": <str>,\n",
    "            \"customer_name\": <str>, \"insurance_type\": <int>, \"issue_summary\": <str>, \"query_category\": <str>,\n",
    "            \"query_subcategory\": <str>, \"urgency_score\": <int>, \"recommended_message\": <str>}\n",
    "        \"\"\"\n",
    "        # Extract data from the waiting list\n",
    "        try:\n",
    "            with open(self.waiting_list_path, \"r\") as f:\n",
    "                waiting_list = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            waiting_list = []\n",
    "\n",
    "        # Assume the list is ordered by urgency (first element is the most urgent one)\n",
    "        index = 0\n",
    "        for i in range(len(waiting_list)):\n",
    "            # As a tiebreaker, customers who have been waiting the longest have priority.\n",
    "            if issue_data[\"urgency_score\"] <= waiting_list[i][\"urgency_score\"]:\n",
    "                index += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        waiting_list.insert(index, issue_data)\n",
    "        with open(self.waiting_list_path, \"w\") as f:\n",
    "            json.dump(waiting_list, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "    def run(self, chat_history:str, sentiment:dict, model=\"llama-3.3-70b-versatile\"):\n",
    "        \"\"\"\n",
    "        Extracts information from the chat history (customer name, insurance_type,\n",
    "        issue summary, issue category and subcategory). After that, generates a\n",
    "        recommended message for the human attendant to send to the customer.\n",
    "\n",
    "        Args:\n",
    "            chat_history (str): The history of the conversation with the customer.\n",
    "            sentiment (dict): Sentiment analysis scores.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the extracted information and the recommended message\n",
    "            in the following format:\n",
    "              {\"sentiment\": <dict>, \"chat_history\": <str>, \"human_attendant_name\": <str>,\n",
    "              \"human_attendant_id\": <int>, \"model\": <str>, \"customer_name\": <str>, \"insurance_type\": <int>,\n",
    "              \"issue_summary\": <str>, \"query_category\": <str>, \"query_subcategory\": <str>,\n",
    "              \"urgency_score\": <int>, \"recommended_message\": <str>}\n",
    "        \"\"\"\n",
    "        output = {}\n",
    "        output[\"sentiment\"] = sentiment\n",
    "        output[\"chat_history\"] = chat_history\n",
    "        output[\"model\"] = model\n",
    "\n",
    "        # Get Customer name and insurance type\n",
    "        customer_name_and_ins_type = self.model_answer_name_and_ins_type(chat_history, MODEL)\n",
    "        output[\"customer_name\"] = customer_name_and_ins_type[\"name\"]\n",
    "        output[\"insurance_type\"] = customer_name_and_ins_type[\"insurance_type\"]\n",
    "        self.insurance_type = customer_name_and_ins_type[\"insurance_type\"]\n",
    "\n",
    "        # Get issue summary and query category and query subcategory\n",
    "        sum_cat_and_subcat = self.model_answer_summary_and_category(chat_history, MODEL)\n",
    "        output[\"issue_summary\"] = sum_cat_and_subcat[\"summary\"]\n",
    "        output[\"query_category\"] = sum_cat_and_subcat[\"category\"]\n",
    "        output[\"query_subcategory\"] = sum_cat_and_subcat[\"subcategory\"]\n",
    "        self.query_category = sum_cat_and_subcat[\"category\"]\n",
    "\n",
    "        # Try to find an available agent\n",
    "        available_agent = self.find_available_human_agent()\n",
    "        if available_agent is not None:\n",
    "            output[\"human_attendant_name\"] = available_agent[\"human_attendant_name\"]\n",
    "            output[\"human_attendant_id\"] = available_agent[\"id\"]\n",
    "        else:\n",
    "            output[\"human_attendant_name\"] = \"\"\n",
    "            output[\"human_attendant_id\"] = \"\"\n",
    "\n",
    "\n",
    "        # Calculate urgency score\n",
    "        output[\"urgency_score\"] = self.calculate_urgency(sentiment, sum_cat_and_subcat[\"category\"], sum_cat_and_subcat[\"subcategory\"], customer_name_and_ins_type[\"insurance_type\"])\n",
    "\n",
    "        # Get the recommended message\n",
    "        output[\"recommended_message\"] = self.recommended_message(chat_history, output[\"human_attendant_name\"])\n",
    "\n",
    "        # If there is no available human agent, add customer to the waiting list based on urgency\n",
    "        if output[\"human_attendant_name\"] == \"\":\n",
    "            self.add_to_waiting_list(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3722,
     "status": "ok",
     "timestamp": 1745024808929,
     "user": {
      "displayName": "Matheus Ferracciú Scatolin",
      "userId": "06121676812765815249"
     },
     "user_tz": 180
    },
    "id": "Tb5JLIkZra7o",
    "outputId": "b08def4c-fbbe-48fd-8c28-56a102b82b01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sentiment\": {\n",
      "    \"positive\": 0.1,\n",
      "    \"neutral\": 0.8,\n",
      "    \"negative\": 0.1\n",
      "  },\n",
      "  \"chat_history\": \"I am Doug, from the health insurance. I need to talk to my insurance agent.\",\n",
      "  \"model\": \"llama-3.3-70b-versatile\",\n",
      "  \"customer_name\": \"Doug\",\n",
      "  \"insurance_type\": 2,\n",
      "  \"issue_summary\": \"Doug, a health insurance subscriber, needs to contact his insurance agent.\",\n",
      "  \"query_category\": \"Human Support Escalations\",\n",
      "  \"query_subcategory\": \"Urgent Assistance\",\n",
      "  \"human_attendant_name\": \"Letícia\",\n",
      "  \"human_attendant_id\": 24,\n",
      "  \"urgency_score\": 56,\n",
      "  \"recommended_message\": \"Hello Doug, this is Letícia, your attendant from the health insurance area of Bradesco Seguros. I understand you'd like to discuss something regarding your health insurance, and I'm here to listen and assist you. I want to ensure that you receive the best possible support, so please feel free to share what's on your mind. What would you like to talk about regarding your health insurance today?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "module = Module4_HumanEscalation()\n",
    "\n",
    "# Input:\n",
    "sentiment = {\n",
    "    \"positive\": 0.1,\n",
    "    \"neutral\": 0.8,\n",
    "    \"negative\": 0.1\n",
    "}\n",
    "chat_history = \"I am Doug, from the health insurance. I need to talk to my insurance agent.\"\n",
    "\n",
    "# Output:\n",
    "output = module.run(chat_history, sentiment)\n",
    "print(json.dumps(output, indent=2, ensure_ascii=False))\n",
    "module.free_human_agent(output[\"human_attendant_id\"])\n",
    "\n",
    "# Save to the waiting list (just to demonstrate)\n",
    "module.add_to_waiting_list(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AHa-S-hG7tl"
   },
   "source": [
    "## 2.5 Module 5: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mN1ZTVJFG9oF"
   },
   "outputs": [],
   "source": [
    "class Module5_SentimentAnalysis(Module):\n",
    "    def __init__(self, model_name:str=MODEL,\n",
    "                 sentiment_model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"):\n",
    "        super().__init__(model_name)\n",
    "        self.sentiment_model = sentiment_model\n",
    "\n",
    "    def sentiment_analysis(self, text: str) -> dict[str, float]:\n",
    "        \"\"\"\n",
    "        Analyzes the sentiment of a given text and returns sentiment scores.\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "        Returns:\n",
    "            dict[str, float]: Sentiment scores for positive, neutral, and negative classes.\n",
    "        Example: {\"positive\": 0.03, \"neutral\": 0.92, \"negative\": 0.05}\n",
    "        \"\"\"\n",
    "\n",
    "        # Define tokenizer, config and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.sentiment_model)\n",
    "        config = AutoConfig.from_pretrained(self.sentiment_model)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(self.sentiment_model)\n",
    "\n",
    "        # Tokenize input text\n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "        # Perform sentiment analysis\n",
    "        output = model(**encoded_input)\n",
    "        scores = softmax(output[0][0].detach().numpy())\n",
    "\n",
    "        # Map scores to sentiment labels\n",
    "        return {\"positive\": float(scores[2]), \"neutral\": float(scores[1]), \"negative\": float(scores[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "b431ef42a5b943ae99cd8ec2e1346fa3",
      "3aa40b4cf93a480d8eb6ba521cac5b38",
      "5ee1dd5c90f64ab796cbe2a527c6ef6d",
      "883d0e4793ff4ca88dd5d8726f376eb4",
      "d9e3559554384d789e58e674eab5e68a",
      "e7db4e5de2e24acf9129145a7d3eeea2",
      "8a64085c48b0415ea83abec0bb5307c3",
      "2223c00551dc4477825363155ab65c9f",
      "c429c9c56aca43478b7fa9d4b597615b",
      "19b0bb6a1aa0429db3a7ea38d899face",
      "43a54bb85c9b469d9ac8f79a60fc1058",
      "bd8ef5a799994e13b3b2a0cff6a4d35e",
      "b1b3f3fcb3df45d7b4a243620a892f6d",
      "7bfc833a8b4d4a2fa835a72ba37f315f",
      "1d259b3d51034b1ea333f2630be837f2",
      "94816008614e41dd838bbb29825beb83",
      "7bff38cce8fc463696cea8457eee3c83",
      "0104a7801822463e8f2f316961e99853",
      "98767ce65b56428ea358508ed234fd9f",
      "7a8113275d9045388da9773d684264d3",
      "6f748b2b3a2045e4914e34b3eab53728",
      "0d7df54542c0481aa0c25897fd792f1f",
      "82ba5dafec9f4477b836547c5781f6fd",
      "93893f0cff86483aa353b865bbb6909e",
      "2f37c3e77f68456cae4eb44b4765cf1b",
      "aa7b1f3102774d248dd66928ea683475",
      "ba25558a159145f992f92d62b1c5be2b",
      "2eb48e5a5bb2456c93900bb0db59d8ee",
      "0039cd3d791c4788a9551e5beae0313f",
      "977df3b787fb4cad9c742bec9fec941e",
      "2614da0642314fb78f9ffda518fa8594",
      "ccc2d3d9e048454caac2330bd48c42da",
      "bbeadfa4141b436da0751ef6bcf9f61c",
      "f03e0238fba24c60a51b6330e2f012f3",
      "d595ca2d3a814d5884509fc3a7b40da1",
      "71fbe5f487ff43108820d7f8308260ac",
      "d775df2069cf4866beedc37b42fabcaa",
      "5213c48241dd4088999b44d75db75db5",
      "1d3b7f8725ac43cca63189dfc4f07f37",
      "f05660227f7b44da8ea766212b609693",
      "b1c5e93cd6ec486d835126feaf944e39",
      "8f171fb16aac40cc8ddeacffdedd5935",
      "29a8d8fd63194af5814e066b831170af",
      "e9d431cf04044006a614bfce5c7ce4dc",
      "2ec5414403b94ee48cdf18cee7ee0570",
      "34a97ddd66984277806ae27b83a3880e",
      "71d4d78674bf4ae9aca5d6ec4ec8bc6f",
      "8806caf1f786422587e6d7a0121465ad",
      "63a51f3ed96d4a59b53a92e25c47c500",
      "01245514bea24d659f578a0fb52666fe",
      "32cfe374270042a2a4a10cf6f8d7d74c",
      "05475ef0002a48febcfc9304568e3c70",
      "807215aa351c46b59a05fa8b1733a1ef",
      "d0d9f36953e54c97ad1b41e6a393031d",
      "94dbf4e197d44598a0c06df3b9d08bf2"
     ]
    },
    "executionInfo": {
     "elapsed": 6322,
     "status": "ok",
     "timestamp": 1745024815298,
     "user": {
      "displayName": "Matheus Ferracciú Scatolin",
      "userId": "06121676812765815249"
     },
     "user_tz": 180
    },
    "id": "tStKwnD9IpYT",
    "outputId": "d7a57a82-5efd-438e-b768-39ddd31c6f5c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b431ef42a5b943ae99cd8ec2e1346fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8ef5a799994e13b3b2a0cff6a4d35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ba5dafec9f4477b836547c5781f6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03e0238fba24c60a51b6330e2f012f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec5414403b94ee48cdf18cee7ee0570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 0.019222112372517586, 'neutral': 0.7683683037757874, 'negative': 0.2124096155166626}\n"
     ]
    }
   ],
   "source": [
    "module = Module5_SentimentAnalysis()\n",
    "text = \"How can I cancel my insurance?\"\n",
    "\n",
    "print(module.sentiment_analysis(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8z8ygPWMQvx"
   },
   "source": [
    "## 2.6 Module 6: Feedback Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ioaaQgGnMZvZ"
   },
   "outputs": [],
   "source": [
    "comment_mapping = {\n",
    "    \"Desempenho do Modulee\": \"Module Performance\",\n",
    "    \"Tempo de Resposta\": \"Response Time\",\n",
    "    \"Problema de Usabilidade\": \"Usability Issue\",\n",
    "    \"Problema de Acessibilidade\": \"Accessibility Issue\",\n",
    "    \"Problema na Apólice de Seguro\": \"Insurance Policy Issue\",\n",
    "    \"Problema no Processamento de Sinistros\": \"Claims Processing Issue\",\n",
    "    \"Problema de Cobrança ou Pagamento\": \"Payment or Billing Issue\",\n",
    "    \"Erro no Sistema\": \"System Error\",\n",
    "    \"Solicitação de Recurso\": \"Resource Request\",\n",
    "    \"Sugestão de Melhoria de Processo\": \"Improvement Suggestion in Process\",\n",
    "    \"Experiência Positiva\": \"Positive Experience\",\n",
    "    \"Consulta Geral\": \"General Inquiry\",\n",
    "    \"Feedback Diverso\": \"Diverse Feedback\",\n",
    "    \"Outros\": \"Other\"\n",
    "}\n",
    "\n",
    "class Module6_FeedbackCollector(Module):\n",
    "    def __init__(self, chat_history:str, human_attendant_name:str, sentiment_analysis:dict,\n",
    "                 insurance_type:int, issue_summary:str, query_category:str, query_subcategory:str,\n",
    "                 model_name:str=MODEL, requirements_path:str=f\"{STELLAR_path}/requirements/module_6\",\n",
    "                 comments_path:str=f\"{STELLAR_path}/outputs/module_6/comments\"):\n",
    "        super().__init__(model_name)\n",
    "        self.chat_history = chat_history\n",
    "        self.human_attendant_name = human_attendant_name\n",
    "        self.sentiment_analysis = sentiment_analysis\n",
    "        self.insurance_type = insurance_type\n",
    "        self.issue_summary = issue_summary\n",
    "        self.query_category = query_category\n",
    "        self.query_subcategory = query_subcategory\n",
    "        self.requirements_path = requirements_path\n",
    "        self.comments_path = comments_path\n",
    "\n",
    "        # Load info from the requirements path\n",
    "        self.category_to_team = self.load_json_file(f\"{requirements_path}/category_to_team.json\")\n",
    "        self.categorization_prompt = self.load_txt_file(f\"{requirements_path}/categorization_prompt.txt\")\n",
    "        self.feedback_questions = self.load_json_file(f\"{requirements_path}/feedback_questions.json\")\n",
    "        self.keys = self.load_json_file(f\"{requirements_path}/keys.json\")\n",
    "        self.feedback_categories = self.load_json_file(f\"{requirements_path}/feedback_categories.json\")\n",
    "\n",
    "\n",
    "    def categorize_comment(self, customer_comment: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Categorizes a customer comment into feedback categories.\n",
    "\n",
    "        Args:\n",
    "            customer_comment (str): The customer comment to categorize.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of feedback categories.\n",
    "        \"\"\"\n",
    "        prompt = f\"{self.categorization_prompt}\\nInput: {customer_comment}\\nOutput: \"\n",
    "        response = self.ask_model(prompt, self.model_name)\n",
    "        categories = []\n",
    "        for category in self.feedback_categories:\n",
    "            if category in response:\n",
    "                categories.append(category)\n",
    "\n",
    "        if len(categories) > 0:\n",
    "            return categories\n",
    "        return [\"Other\"]\n",
    "\n",
    "\n",
    "    def comment_routing_and_saving(self, feedback: dict):\n",
    "        \"\"\"\n",
    "        Routes and saves customer feedback based on their categories.\n",
    "\n",
    "        Args:\n",
    "            feedback (dict): A dictionary of customer feedback.\n",
    "        \"\"\"\n",
    "        # Get the time\n",
    "        time = self.get_current_date_time()\n",
    "\n",
    "        # Write the team-specific feedback\n",
    "        for key, value in feedback.items():\n",
    "            if \"categories\" in value and value[\"categories\"]:\n",
    "                for category in value[\"categories\"]:\n",
    "                    team = self.category_to_team.get(category, \"Customer Support Team\")\n",
    "                    filename = f\"{self.comments_path}/{team.replace(' ', '_').lower()}_feedback.json\"\n",
    "                    entry = {\"feedback\": value,\"feedback_type\":key, \"time\": time}\n",
    "\n",
    "                    try:\n",
    "                        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "                            data = json.load(f)\n",
    "                    except FileNotFoundError:\n",
    "                        data = []\n",
    "\n",
    "                    # Add the new feedback in te beggining of the list, so that newer feedback appears first\n",
    "                    data.insert(0, entry)\n",
    "\n",
    "                    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Write the overall feedback: chat_history, human_attendant_name, sentiment_analysis, customer_name, insurance_type, issue_summary, query_category, query_subcategory\n",
    "        path = f\"{self.comments_path}/overall_feedback.json\"\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            data = []\n",
    "\n",
    "        entry = {\"feedback\": feedback, \"time\": time}\n",
    "        entry[\"chat_history\"] = self.chat_history\n",
    "        if self.human_attendant_name != \"\":\n",
    "          entry[\"human_attendant_name\"] = self.human_attendant_name\n",
    "        if self.sentiment_analysis != {}:\n",
    "          entry[\"sentiment_analysis\"] = self.sentiment_analysis\n",
    "        entry[\"insurance_type\"] = self.insurance_type\n",
    "        if self.issue_summary != \"\":\n",
    "          entry[\"issue_summary\"] = self.issue_summary\n",
    "        if self.query_category != \"\":\n",
    "          entry[\"query_category\"] = self.query_category\n",
    "        if self.query_subcategory != \"\":\n",
    "          entry[\"query_subcategory\"] = self.query_subcategory\n",
    "\n",
    "        data.insert(0, entry)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def get_customer_feedback(self) -> dict:\n",
    "        \"\"\"\n",
    "        Collects customer feedback by asking multiple-choice questions (1-5 scale) and optional follow-up questions.\n",
    "\n",
    "        Returns:\n",
    "            dict: Feedback containing ratings, optional follow-up responses for each question in this format:\n",
    "            {\n",
    "                key: {\n",
    "                    \"rating\": <int>,\n",
    "                    \"follow_up_response\": <str>,\n",
    "                    \"categories\": [<str>]\n",
    "                },\n",
    "        \"\"\"\n",
    "        feedback = {}\n",
    "\n",
    "        for key, question in zip(self.keys, self.feedback_questions):\n",
    "            while True:\n",
    "                try:\n",
    "                    rating = int(input(f\"{question} (1-5): \"))\n",
    "                    if rating < 1 or rating > 5:\n",
    "                        print(\"Please, enter a number between 1 and 5.\")\n",
    "                        continue\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Please, enter a number between 1 and 5.\")\n",
    "\n",
    "            follow_up_response = None\n",
    "            if rating <= 3:\n",
    "                follow_up_response = input(\n",
    "                    \"We are sorry to hear that. Could you please tell us what could have been better? \")\n",
    "\n",
    "            feedback[key] = {\n",
    "                \"rating\": rating,\n",
    "                \"follow_up_response\": follow_up_response\n",
    "            }\n",
    "\n",
    "        # the general rating is the average of the other ratings\n",
    "        general_rating = round(sum([feedback[key][\"rating\"] for key in self.keys]) / len(self.keys))\n",
    "        # Ask for optional general comment\n",
    "        comment = input(\"Would you like to provide more general comments about the service? \")\n",
    "        feedback[\"general\"] = {\"rating\": general_rating, \"follow_up_response\": comment}\n",
    "\n",
    "        # Categorize each written comment\n",
    "        for key, value in feedback.items():\n",
    "          if value[\"follow_up_response\"] is not None and len(value[\"follow_up_response\"]) > 5:\n",
    "            feedback[key][\"categories\"] = self.categorize_comment(value[\"follow_up_response\"])\n",
    "          else:\n",
    "            feedback[key][\"categories\"] = []\n",
    "\n",
    "        # route comments\n",
    "        self.comment_routing_and_saving(feedback)\n",
    "\n",
    "        return feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "becfd0b045a441e88bfa3e97096ceb12",
      "b6b1597d63c74bcea5161b9c406e8355",
      "293cf00fd50f4be4a3620df3b598c2e0",
      "8cfdb215d16c42818a19d04b161a71ec",
      "24e5eb8de1ee414fa591c0b81fb91ca7",
      "77ef0d5138814183b0359358452540a9",
      "7dca507d9367407b8795430c532dc3be",
      "536e2063c8b0474da8ca646dfb9d1e6b",
      "a759ca3477e34f20ba8d005c4237808b",
      "cb9ea495d5f54192a8dc056cb265b7aa",
      "826cf2174ff144e7bfe0d824788416b1"
     ]
    },
    "executionInfo": {
     "elapsed": 1361,
     "status": "ok",
     "timestamp": 1745024816667,
     "user": {
      "displayName": "Matheus Ferracciú Scatolin",
      "userId": "06121676812765815249"
     },
     "user_tz": 180
    },
    "id": "2h6lnPAKOnsf",
    "outputId": "a8078e67-ca9a-4aaa-d4db-1629d404f01f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becfd0b045a441e88bfa3e97096ceb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_history = \"Hello, I am Matheus. I am very frustrated with the delay in the action of my health insurance.\"\n",
    "human_attendant_name = \"Gabriel\"\n",
    "sentiment = {\n",
    "    \"positive\": 0.022862698882818222,\n",
    "    \"neutral\": 0.031627094745636,\n",
    "    \"negative\": 0.945510176569223404\n",
    "}\n",
    "insurance_type = 2\n",
    "issue_summary = \"Matheus needs help with the health insurance action.\"\n",
    "query_category = \"Claims\"\n",
    "query_subcategory = \"Opening of Claim\"\n",
    "\n",
    "module = Module6_FeedbackCollector(chat_history, human_attendant_name, sentiment, insurance_type, issue_summary, query_category, query_subcategory)\n",
    "#feedback = module.get_customer_feedback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Suddcf6bRY9z"
   },
   "source": [
    "## 2.7 Module 7: Knowledge Base Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2elNw2-9RjUH"
   },
   "outputs": [],
   "source": [
    "class Module7_KnowledgeBaseBuilder(Module):\n",
    "    def __init__(self, chat_history:str, insurance_type:int, model_name:str=MODEL, prompt_path:str=f\"{STELLAR_path}/requirements/module_7/prompt.txt\",\n",
    "                 review_query_path:str=f\"{STELLAR_path}/outputs/module_7/review_query.json\",\n",
    "                 int_to_ins_type_path:str=f\"{STELLAR_path}/requirements/module_7/int_to_ins_type.json\"):\n",
    "        super().__init__(model_name)\n",
    "        self.chat_history = chat_history\n",
    "        self.insurance_type = insurance_type\n",
    "        self.model_name = model_name\n",
    "        self.review_query_path = review_query_path\n",
    "\n",
    "        self.prompt = self.load_txt_file(prompt_path)\n",
    "        self.int_to_ins_type = self.load_json_file(int_to_ins_type_path)\n",
    "\n",
    "\n",
    "    def add_to_review_queue(self, draft_faq_entry: dict) -> None:\n",
    "        \"\"\"\n",
    "        Adds generated draft FAQ entries to a queue for human review.\n",
    "\n",
    "        Args:\n",
    "          draft_faq_entry (list): A list of dictionaries representing draft FAQ entries.\n",
    "        \"\"\"\n",
    "        try:\n",
    "          with open(self.review_query_path, \"r\") as f:\n",
    "              review_query = json.load(f)\n",
    "        except:\n",
    "          print(\"Error loading existing review query. Creating new one.\")\n",
    "          review_query = []\n",
    "\n",
    "        time = self.get_current_date_time()\n",
    "\n",
    "        new_faq_entry = {\n",
    "            \"draft_faq\": draft_faq_entry,\n",
    "            \"chat_history\": self.chat_history,\n",
    "            \"time\": time,\n",
    "            \"status\": \"pending\"\n",
    "        }\n",
    "        review_query.append(new_faq_entry)\n",
    "        try:\n",
    "          with open(self.review_query_path, \"w\") as f:\n",
    "              json.dump(review_query, f, indent=4, ensure_ascii=False)\n",
    "        except:\n",
    "            print(\"Error writing to review query file. Changes not saved.\")\n",
    "\n",
    "\n",
    "    def parse_response(self, response:str)->dict:\n",
    "        \"\"\"\n",
    "        Parses the response from the model and extracts a dict of fields \"question\"\n",
    "        and \"draft_answer\". If the answer is not in json format or the fields are\n",
    "        not present, the function raises an exception.\n",
    "\n",
    "        Args:\n",
    "            response (str): The response from the model as a string.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the parsed fields.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the response is not in JSON format or the fields are missing.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            json_data = json.loads(response)\n",
    "            if \"question\" not in json_data or \"draft_answer\" not in json_data:\n",
    "                raise Exception(\"Missing 'question' or 'draft_answer' field in LLM response.\")\n",
    "            if not isinstance(json_data[\"question\"], str) or not isinstance(json_data[\"draft_answer\"], str):\n",
    "                raise Exception(\"Invalid data type for 'question' or 'draft_answer' field.\")\n",
    "            if not json_data[\"question\"].strip() or not json_data[\"draft_answer\"].strip():\n",
    "                raise Exception(\"Empty 'question' or 'draft_answer' field.\")\n",
    "            if len(json_data) != 2:\n",
    "                raise Exception(\"LLM response has more than 2 fields.\")\n",
    "            return json_data\n",
    "        except json.JSONDecodeError:\n",
    "            # if the response has more than 100 characters, truncate it\n",
    "            if len(response) > 100:\n",
    "                response = response[:100] + \"...\"\n",
    "            print(\"Response: \" + response)\n",
    "            raise Exception(\"LLM response is not in JSON format.\")\n",
    "\n",
    "\n",
    "    def generate_draft_faq(self)->dict:\n",
    "        \"\"\"\n",
    "        Creates a new draft FAQ entry in the database. Allow a maximum of 2 retries.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the fields \"question\" and \"draft_answer\".\n",
    "            If the draft FAQ cannot be generated after 3 attempts, returns None.\n",
    "        \"\"\"\n",
    "        retries = 0\n",
    "        while retries < 3:\n",
    "            try:\n",
    "                prompt = self.prompt + f\"\\n{self.chat_history}\"\n",
    "                response = self.ask_model(prompt, self.model_name)\n",
    "                draft_faq = self.parse_response(response)\n",
    "                draft_faq[\"category\"] = self.int_to_ins_type.get(str(self.insurance_type), \"default\")\n",
    "\n",
    "                self.add_to_review_queue(draft_faq)\n",
    "                return draft_faq\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                if retries == 3:\n",
    "                    raise Exception(f\"Failed to generate draft FAQ after {retries} attempts: {e}\")\n",
    "                print(f\"Error generating draft FAQ: {e}. Retrying...\")\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def review_pending_faqs(self) -> None:\n",
    "        \"\"\"\n",
    "        Reviews pending FAQ entries and allows the human agent to approve, disapprove, rewrite, or leave them as pending.\n",
    "        Approved or disapproved FAQs are saved in the respective files and removed from the review queue.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.review_query_path, \"r\") as f:\n",
    "                review_query = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(\"No pending FAQs found.\")\n",
    "            return\n",
    "\n",
    "        updated_review_query = []\n",
    "        for entry in review_query:\n",
    "            if entry[\"status\"] != \"pending\":\n",
    "                updated_review_query.append(entry)\n",
    "                continue\n",
    "\n",
    "            print(\"FAQ review:\")\n",
    "            print(f\"Question: {entry['draft_faq']['question']}\")\n",
    "            print(f\"Draft Answer: {entry['draft_faq']['draft_answer']}\")\n",
    "            print(f\"Category: {entry['draft_faq']['category']}\")\n",
    "            print(\"\\nOptions\")\n",
    "            print(\"a) Rewrite the answer.\")\n",
    "            print(\"b) Approve FAQ without changes.\")\n",
    "            print(\"c) Disapprove the FAQ.\")\n",
    "            print(\"d) Skip to the next FAQ (and leave it pending).\")\n",
    "            decision = input(\"Enter your choice (a/b/c/d): \").strip().lower()\n",
    "\n",
    "            if decision == \"a\":\n",
    "                new_answer = input(\"Enter your answer to the question: \").strip()\n",
    "                entry[\"draft_faq\"][\"draft_answer\"] = new_answer\n",
    "                entry[\"status\"] = \"approved\"\n",
    "                approved_path = self.review_query_path.replace(\"review_query\", \"approved_faqs\")\n",
    "                self.save_faq_and_upload_chroma(entry, approved_path)\n",
    "\n",
    "\n",
    "            elif decision == \"b\":\n",
    "                entry[\"status\"] = \"approved\"\n",
    "                approved_path = self.review_query_path.replace(\"review_query\", \"approved_faqs\")\n",
    "                self.save_faq_and_upload_chroma(entry, approved_path)\n",
    "\n",
    "            elif decision == \"c\":\n",
    "                entry[\"status\"] = \"rejected\"\n",
    "                rejected_path = self.review_query_path.replace(\"review_query\", \"rejected_faqs\")\n",
    "                self.save_faq_and_upload_chroma(entry, rejected_path)\n",
    "\n",
    "            elif decision == \"d\":\n",
    "                print(\"Keeping the status as pending.\")\n",
    "                updated_review_query.append(entry)\n",
    "\n",
    "            else:\n",
    "                print(\"Invalid input. Keeping the status as pending.\")\n",
    "                updated_review_query.append(entry)\n",
    "\n",
    "        with open(self.review_query_path, \"w\") as f:\n",
    "            json.dump(updated_review_query, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    def save_faq_and_upload_chroma(self, entry: dict, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Saves the approved or disapproved FAQ to the respective JSON file.\n",
    "        If the FAQ was approved, adds it to the FAQ vector database.\n",
    "\n",
    "        Args:\n",
    "            entry (dict): The FAQ entry to save in this format:\n",
    "                {\"draft_faq\": {\"question\": <str>, \"draft_answer\": <str>, \"category\": <str>}}\n",
    "            path (str): The path to the JSON file where the FAQ should be saved.\n",
    "        \"\"\"\n",
    "        # Save to the Chroma vector database\n",
    "        if entry[\"status\"] == \"approved\":\n",
    "            new_faq = entry[\"draft_faq\"]\n",
    "\n",
    "            formated_faq = {\n",
    "                \"category\": new_faq[\"category\"],\n",
    "                \"question\": new_faq[\"question\"],\n",
    "                \"answer\": new_faq[\"draft_answer\"]\n",
    "            }\n",
    "            self.update_faq_vector_database(formated_faq)\n",
    "\n",
    "        # Save to the json file\n",
    "        try:\n",
    "            with open(path, \"r\") as f:\n",
    "                faqs = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            faqs = []\n",
    "\n",
    "        faqs.append(entry)\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(faqs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "    def update_faq_vector_database(self, approved_faq: dict) -> None:\n",
    "        \"\"\"\n",
    "        Updates the FAQ vector database with a single new approved FAQ by calling Module2's add_new_faq_to_chroma function.\n",
    "\n",
    "        Args:\n",
    "            approved_faq (dict): The approved FAQ entry to add to the vector database in this format:\n",
    "                {\"category\": <str>, \"question\": <str>, \"draft_answer\": <str> }.\n",
    "        \"\"\"\n",
    "        print(\"Updating FAQ vector database...\")\n",
    "        module_2 = Module2_RAG()\n",
    "        module_2.add_new_faq_to_chroma(approved_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1691,
     "status": "ok",
     "timestamp": 1745024818421,
     "user": {
      "displayName": "Matheus Ferracciú Scatolin",
      "userId": "06121676812765815249"
     },
     "user_tz": 180
    },
    "id": "NfuWIy0vx_JT",
    "outputId": "d7926d21-b1ab-44e7-9883-cc46b47821f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"question\": \"How do I find licensed dentists near me to use my dental insurance?\",\n",
      "    \"draft_answer\": \"You can find licensed dentists near you through our website or by contacting Bradesco Insurance's Customer Service Center for a list of providers in your area.\",\n",
      "    \"category\": \"dental_insurance\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chat_history = \"I need to activate my dental insurance, but I don't know how to find licensed dentists near me.\"\n",
    "ins_type = 4\n",
    "\n",
    "module = Module7_KnowledgeBaseBuilder(chat_history, ins_type)\n",
    "draft_faq = module.generate_draft_faq()\n",
    "print(json.dumps(draft_faq, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKWQxXXtyFut"
   },
   "outputs": [],
   "source": [
    "module_2 = Module2_RAG()\n",
    "new_draft_faq = {\n",
    "    \"question\": \"How do I activate my travel insurance in case of missing luggage?\",\n",
    "    \"answer\": \"You can access our app, go to 'my insurance'. 'travel insurance' and 'I lost my luggage'.\",\n",
    "    \"category\": \"travel_insurance\"\n",
    "}\n",
    "#module_2.add_new_faq_to_chroma(new_draft_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q16F0JsPy9Rq"
   },
   "outputs": [],
   "source": [
    "module_7 = Module7_KnowledgeBaseBuilder(chat_history =\"\", insurance_type=None)\n",
    "#module_7.review_pending_faqs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOsKCQFazH5q"
   },
   "source": [
    "## 2.8. Module 8: Resolution Verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YL28ziZGziPc"
   },
   "outputs": [],
   "source": [
    "class Module8_ResolutionVerifier(Module):\n",
    "    def __init__(self, chat_history:str, model_name:str=MODEL, prompt_path:str=f\"{STELLAR_path}/requirements/module_8/prompt_resolution_verifier_question.txt\"):\n",
    "        super().__init__(model_name)\n",
    "        self.chat_history = chat_history\n",
    "        self.prompt_path = prompt_path\n",
    "        self.prompt = self.load_txt_file(self.prompt_path)\n",
    "\n",
    "    def generate_verification_question(self) -> str:\n",
    "        \"\"\"\n",
    "        Generates a tailored verification question for the customer based on the parsed chat history.\n",
    "\n",
    "        Returns:\n",
    "            str: A personalized question to verify if the issue was resolved.\n",
    "        \"\"\"\n",
    "        prompt = self.prompt.format(chat_history=self.chat_history)\n",
    "        return self.ask_model(prompt, self.model_name)\n",
    "\n",
    "    def verify_issue_resolution(self, verification_question:str) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Verifies with the customer if the issue was resolved based on the provided verification question.\n",
    "\n",
    "        Args:\n",
    "            verification_question (str): The question to verify if the issue was resolved.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the issue was resolved, False otherwise.\n",
    "            str: The chat history from the module with the verification question and answer.\n",
    "        \"\"\"\n",
    "        chat_history = \"\\n\\033[34mModule 8\\033[0m: \" + verification_question\n",
    "        answer = \"\"\n",
    "        while answer != \"Y\" and answer != \"N\":\n",
    "            answer = input(verification_question + \" (Y/N): \\n\")\n",
    "            answer = answer.upper().strip()[0]\n",
    "            if answer != \"Y\" and answer != \"N\":\n",
    "                print(\"Invalid response. Please respond with 'Y' or 'N' only.\")\n",
    "\n",
    "        chat_history += \"\\n\\033[32mUser\\033[0m: \" + answer\n",
    "        return answer == \"Y\", chat_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W725t5Fj0QCh"
   },
   "outputs": [],
   "source": [
    "chat_history = \"\"\"\"Hello, I'm José. I have a question about activating my car insurance. Where can I find the activation option?\n",
    "Hi, José! I'm the virtual assistant for Bradesco Seguros. Based on a search of our database, in your case, you should access the Bradesco Seguros App and go to the \"my insurance\" page, where you will find information about activating your car insurance.\"\"\"\n",
    "module = Module8_ResolutionVerifier(chat_history=chat_history)\n",
    "#is_issue_resolved, new_ch = module.verify_issue_resolution(module.generate_verification_question())\n",
    "#print(is_issue_resolved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJCjG7U70iAZ"
   },
   "source": [
    "## 2.9 Module 9: Compliance Verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZSAPOk40hZ8"
   },
   "outputs": [],
   "source": [
    "class Module9_ComplianceVerifier(Module):\n",
    "    def __init__(self, user_question:str, llm_response:str, model_name:str=\"gemma2-9b-it\",\n",
    "                 prompt_path:str=f\"{STELLAR_path}/requirements/module_9/prompt_compliance_verifier_1.txt\",\n",
    "                 log_path:str=f\"{STELLAR_path}/outputs/module_9/logs/violations.json\"):\n",
    "        super().__init__(model_name)\n",
    "        self.user_question = user_question\n",
    "        self.llm_response = llm_response\n",
    "        self.prompt_path = prompt_path\n",
    "        self.log_path = log_path\n",
    "        self.prompt = self.load_prompt()\n",
    "\n",
    "    def load_prompt(self):\n",
    "        with open(self.prompt_path, \"r\") as f:\n",
    "            prompt = f.read()\n",
    "        return prompt\n",
    "\n",
    "    def format_input(self) -> str:\n",
    "        \"\"\"\n",
    "        Formats the input for the LLM by combining the chat history, LLM response, and the loaded prompt.\n",
    "        Returns the formatted input string ready to be sent to the model.\n",
    "        \"\"\"\n",
    "        dict_input = {\"query\":self.user_question, \"response\":self.llm_response}\n",
    "        prompt = self.prompt + json.dumps(dict_input, indent=2, ensure_ascii=False)\n",
    "        return prompt\n",
    "\n",
    "    def parse_response(self, llm_output: str) -> dict:\n",
    "        \"\"\"\n",
    "        Parses the output from the model and extracts a dict of fields \"compliance\"\n",
    "        and \"violation\". If the answer is not in json format or the fields are\n",
    "        not present, the function raises an exception.\n",
    "\n",
    "        Args:\n",
    "            llm_output (str): The output from the model as a string.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the parsed fields.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the response is not in JSON format or the fields are missing.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if llm_output is None:\n",
    "                raise Exception(\"LLM output is None.\")\n",
    "            dict_response = llm_output[llm_output.index(\"{\"):llm_output.index(\"}\")+1]\n",
    "            json_data = json.loads(dict_response)\n",
    "            if \"compliance\" not in json_data or \"violation\" not in json_data:\n",
    "                raise Exception(\"Missing 'compliance' or 'violation' field in LLM response.\")\n",
    "            if not isinstance(json_data[\"compliance\"], bool) or not isinstance(json_data[\"violation\"], str):\n",
    "                raise Exception(\"Invalid data type for 'compliance' or 'violation' field.\")\n",
    "            if not json_data[\"violation\"].strip():\n",
    "                raise Exception(\"Empty 'violation' field.\")\n",
    "            if len(json_data) != 2:\n",
    "                raise Exception(\"LLM response has more than 2 fields.\")\n",
    "            return json_data\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Response: \" + llm_output)\n",
    "            raise Exception(\"LLM response is not in JSON format.\")\n",
    "\n",
    "\n",
    "    def run_verification(self) -> dict:\n",
    "        \"\"\"\n",
    "        Executes the compliance verification process.\n",
    "        Combines input formatting, sending the request to the model, and analyzing the response.\n",
    "        Returns:\n",
    "            dict: The final compliance analysis, including compliance status and violations in this format:\n",
    "                {\"compliance\": <bool>, \"violation\": <str>}\n",
    "\n",
    "        \"\"\"\n",
    "        input = self.format_input()\n",
    "        llm_output = self.ask_model(input, self.model_name)\n",
    "        for i in range(3):\n",
    "            if llm_output is None:\n",
    "                print(f\"LLM output is None, retrying... ({i+1}/3)\")\n",
    "                continue\n",
    "            try:\n",
    "                results = self.parse_response(llm_output)\n",
    "                self.log_violations(results)\n",
    "                return results\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing response: {e}\")\n",
    "\n",
    "        return {\"compliance\": False, \"violation\": f\"ERROR: unable to parse the LLM response: {llm_output}\"}\n",
    "\n",
    "    def log_violations(self, results: dict) -> None:\n",
    "        \"\"\"\n",
    "        Logs any detected violations to a specified log file.\n",
    "        Args:\n",
    "            results (dict): The compliance verification results, including any violations.\n",
    "        \"\"\"\n",
    "        # If there is not a violation, return\n",
    "        if results[\"compliance\"]:\n",
    "            return\n",
    "\n",
    "        # If there is a violation, log it\n",
    "        # Set the time\n",
    "        time = self.get_current_date_time()\n",
    "\n",
    "        # Open the log file\n",
    "        try:\n",
    "          with open(self.log_path, \"r\") as f:\n",
    "              log_data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            log_data = []\n",
    "\n",
    "        log = {\n",
    "            \"id\": len(log_data) + 1,\n",
    "            \"time\": time,\n",
    "            \"query\": self.user_question,\n",
    "            \"response\": self.llm_response,\n",
    "            \"violation\": results[\"violation\"]\n",
    "        }\n",
    "\n",
    "        log_data.append(log)\n",
    "\n",
    "        # Save the log data\n",
    "        with open(self.log_path, \"w\") as f:\n",
    "            json.dump(log_data, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1223,
     "status": "ok",
     "timestamp": 1745024819975,
     "user": {
      "displayName": "Matheus Ferracciú Scatolin",
      "userId": "06121676812765815249"
     },
     "user_tz": 180
    },
    "id": "cZXj77VH3NIn",
    "outputId": "c8ab00ea-af87-4c25-f83e-f8fec680315b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'compliance': False, 'violation': 'Incomplete or vague response'}\n",
      "{'compliance': True, 'violation': 'No violation'}\n",
      "{'compliance': False, 'violation': 'Inappropriate tone'}\n"
     ]
    }
   ],
   "source": [
    "module = Module9_ComplianceVerifier(user_question=\"How can I cancel my dental insurance?\", llm_response=\"Unfortunately I don't have the answer to your question.\")\n",
    "print(module.run_verification())\n",
    "\n",
    "module = Module9_ComplianceVerifier(user_question=\"How can I renew my car insurance?\", llm_response=\"Just send an email to renovacoes_auto@bradesco.com.\")\n",
    "print(module.run_verification())\n",
    "\n",
    "module = Module9_ComplianceVerifier(user_question=\"How can I activate my travel insurance?\", llm_response=\"You can figure it out.\")\n",
    "print(module.run_verification())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb5g8uaP3s6l"
   },
   "source": [
    "# 3. System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jW_jE2AX-TzR"
   },
   "source": [
    "## 3.1 CUSTOMER (query workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPcmSxbCLyUM"
   },
   "source": [
    "This is what the customer sees and how he interacts with the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7ogq68YH4R-"
   },
   "source": [
    "### 3.3.1 Workflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tLFHw_A4bIt"
   },
   "outputs": [],
   "source": [
    "def send_message_to_the_user(message):\n",
    "    print(message)\n",
    "\n",
    "def get_user_input(prompt):\n",
    "    return input(prompt)\n",
    "\n",
    "class WorkflowManager:\n",
    "    def __init__(self):\n",
    "        self.chat_history:str = \"\"\n",
    "        self.logs_path = f\"{STELLAR_path}/logs/workflow_logs.json\"\n",
    "        self.sequence_of_modules = []\n",
    "\n",
    "    def save_logs(self, logs):\n",
    "        try:\n",
    "            with open(self.logs_path, \"r\") as f:\n",
    "                logs_data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            logs_data = []\n",
    "\n",
    "        logs_data.append(logs)\n",
    "        with open(self.logs_path, \"w\") as f:\n",
    "            json.dump(logs_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "    def _log_execution(self, module_number, start_time, response):\n",
    "        execution_time = time.time() - start_time\n",
    "        log_entry = {\n",
    "            \"module\": module_number,\n",
    "            \"execution_time\": execution_time,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"response\": response\n",
    "        }\n",
    "        return log_entry\n",
    "\n",
    "    def process_query(self, query):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query (str): customer's initial query\n",
    "        Returns:\n",
    "            dict: a dict with the final state of the program and the execution logs in this format:\n",
    "                {\"chat_history\": <str>,\n",
    "                \"sequence_of_modules\": <list>,\n",
    "                \"final_state\": <dict>,\n",
    "                \"execution_logs\": <list>}\n",
    "        \"\"\"\n",
    "        logs = []\n",
    "        current_state = {}\n",
    "        self.chat_history += f\"\\n\\033[32mUser\\033[0m: {query}\"\n",
    "\n",
    "        # Start with Module 1\n",
    "        self.sequence_of_modules.append(1)\n",
    "        module1 = Module1_InitialClassifier()\n",
    "        start_time = time.time()\n",
    "        initial_classification:int = module1.classify_question(query)['category']\n",
    "        logs.append(self._log_execution(1, start_time, initial_classification))\n",
    "        current_state['classification'] = initial_classification\n",
    "\n",
    "        # Branch based on Module 1's classification\n",
    "        if initial_classification == 0:  # FAQ path\n",
    "            send_message_to_the_user(\"Thanks for your question! We're looking for an answer to it in the FAQ database.\")\n",
    "            self.chat_history += f\"\\n\\033[36mSystem\\033[0m: We're looking for an answer to it in the FAQ database.\"\n",
    "            current_state = self._handle_faq_path(query, current_state, logs)\n",
    "\n",
    "        elif initial_classification == 1:  # Contact info path\n",
    "            send_message_to_the_user(\"Thank you for your question! We are looking for an answer to it in the contact database.\")\n",
    "            self.chat_history += f\"\\n\\033[36mSystem\\033[0m: Thank you for your question! We are looking for an answer to it in the contact database.\"\n",
    "            current_state = self._handle_contact_path(query, current_state, logs)\n",
    "\n",
    "        elif initial_classification == 2:  # Human escalation path\n",
    "            send_message_to_the_user(\"We will forward you to a human agent. Please wait a moment.\")\n",
    "            self.chat_history += \"\\n\\033[36mSystem\\033[0m: We will forward you to a human agent. Please wait a moment.\"\n",
    "            current_state = self._handle_human_escalation_path(query, current_state, logs)\n",
    "        else: # Question was tagged as not relevant\n",
    "            print(\"Unfortunately, your question was classified as irrelevant. Please try again.\")\n",
    "            return\n",
    "\n",
    "\n",
    "        full_log = {\n",
    "            \"chat_history\": self.chat_history,\n",
    "            \"sequence_of_modules\": self.sequence_of_modules,\n",
    "            \"final_state\": current_state,\n",
    "            \"execution_logs\": logs\n",
    "        }\n",
    "        self.save_logs(full_log)\n",
    "        return full_log\n",
    "\n",
    "    def _handle_faq_path(self, query, state, logs):\n",
    "\n",
    "        # Module 2 - RAG\n",
    "        self.sequence_of_modules.append(2)\n",
    "        module2 = Module2_RAG()\n",
    "        start_time = time.time()\n",
    "        rag_response:str = module2.process_question(query)\n",
    "        logs.append(self._log_execution(2, start_time, rag_response))\n",
    "        state['rag_response'] = rag_response\n",
    "\n",
    "\n",
    "        # Module 9 - Compliance\n",
    "        self.sequence_of_modules.append(9)\n",
    "        module9 = Module9_ComplianceVerifier(query, rag_response)\n",
    "        start_time = time.time()\n",
    "        compliance_check:dict[bool,str] = module9.run_verification()\n",
    "        logs.append(self._log_execution(9, start_time, compliance_check))\n",
    "\n",
    "        if not compliance_check['compliance']:\n",
    "            rag_response = self._handle_compliance_failure_faq_path(query, state, logs)\n",
    "            if rag_response == \"\":\n",
    "                send_message_to_the_user(\"Unfortunately, there was an error searching for your answer in the FAQ database. We will forward you to a human agent.\")\n",
    "                self.chat_history += \"\\n\\033[36mSystem\\033[0m: Unfortunately, there was an error searching for your answer. We will forward you to a human agent.\"\n",
    "                return self._handle_human_escalation_path(query, state, logs)\n",
    "\n",
    "        send_message_to_the_user(rag_response)\n",
    "        self.chat_history += f\"\\n\\033[34mModule 2\\033[0m: {rag_response}\"\n",
    "\n",
    "        # Module 8 - Resolution\n",
    "        self.sequence_of_modules.append(8)\n",
    "        module8 = Module8_ResolutionVerifier(self.chat_history)\n",
    "        start_time = time.time()\n",
    "        verification_question = module8.generate_verification_question()\n",
    "        logs.append(self._log_execution(8, start_time, verification_question))\n",
    "\n",
    "        resolution_check, chat_history_module8 = module8.verify_issue_resolution(verification_question)\n",
    "        self.chat_history += chat_history_module8\n",
    "        state['resolution_check_faq_path'] = resolution_check\n",
    "\n",
    "\n",
    "        if not resolution_check:\n",
    "            send_message_to_the_user(\"It's a shame your issue wasn't resolved. We'll forward you to a human agent.\")\n",
    "            self.chat_history += \"\\n\\033[36mSystem\\033[0m: It's a shame your issue wasn't resolved. We'll forward you to a human agent.\"\n",
    "            return self._handle_human_escalation_path(query, state, logs)\n",
    "\n",
    "        # If the issue is resolved, collect feedback\n",
    "        send_message_to_the_user(\"I'm glad your issue has been resolved! Now, could you please provide us with a quick review to improve our service?\")\n",
    "        self.chat_history += \"\\n\\033[36mSystem\\033[0m: I'm glad your issue has been resolved! Now, could you please provide us with a quick review to improve our service?\"\n",
    "        return self._collect_feedback(query, state, logs)\n",
    "\n",
    "    def _handle_contact_path(self, query, state, logs):\n",
    "        # Module 3 - Contact Info\n",
    "        self.sequence_of_modules.append(3)\n",
    "        module3 = Module3_ContactInfo()\n",
    "        start_time = time.time()\n",
    "        contact_response, chat_history_module3 = module3.ask_module(query)\n",
    "        logs.append(self._log_execution(3, start_time, contact_response))\n",
    "        state['contact_response'] = contact_response\n",
    "        self.chat_history += chat_history_module3\n",
    "\n",
    "        # Module 9 - Compliance\n",
    "        self.sequence_of_modules.append(9)\n",
    "        module9 = Module9_ComplianceVerifier(query, contact_response)\n",
    "        start_time = time.time()\n",
    "        compliance_check:dict[bool,str] = module9.run_verification()\n",
    "        logs.append(self._log_execution(9, start_time, compliance_check))\n",
    "\n",
    "        if not compliance_check['compliance']:\n",
    "            contact_response = self._handle_compliance_failure_contact_info_path(query, state, logs)\n",
    "            if contact_response == \"\":\n",
    "                send_message_to_the_user(\"Unfortunately, there was an error searching for your answer. Let's try to find it in the FAQ database.\")\n",
    "                self.chat_history += \"\\n\\033[36mSystem\\033[0m: Unfortunately, there was an error searching for your answer. Let's try to find it in the FAQ database.\"\n",
    "                return self._handle_faq_path(query, state, logs)\n",
    "\n",
    "        send_message_to_the_user(contact_response)\n",
    "        self.chat_history += f\"\\n\\033[34mModule 3\\033[0m: {contact_response}\"\n",
    "\n",
    "        # Module 8 - Resolution\n",
    "        self.sequence_of_modules.append(8)\n",
    "        module8 = Module8_ResolutionVerifier(self.chat_history)\n",
    "        start_time = time.time()\n",
    "        verification_question = module8.generate_verification_question()\n",
    "        logs.append(self._log_execution(8, start_time, verification_question))\n",
    "        resolution_check, chat_history_module8 = module8.verify_issue_resolution(verification_question)\n",
    "        self.chat_history += chat_history_module8\n",
    "        state['resolution_check_contact_path'] = resolution_check\n",
    "\n",
    "\n",
    "        if not resolution_check:\n",
    "            # Try FAQ path as fallback\n",
    "            send_message_to_the_user(\"It's a shame your question wasn't resolved. We'll forward you to the FAQ search system.\")\n",
    "            self.chat_history += \"\\n\\033[36mSystem\\033[0m: It's a shame your question wasn't resolved. We'll forward you to the FAQ search system.\"\n",
    "            return self._handle_faq_path(query, state, logs)\n",
    "\n",
    "        # If the issue is resolved, collect feedback\n",
    "        send_message_to_the_user(\"I'm glad your issue has been resolved! Now, could you please provide us with a quick review to improve our service?\")\n",
    "        self.chat_history += \"\\n\\033[36mSystem\\033[0m: I'm glad your issue has been resolved! Now, could you please provide us with a quick review to improve our service?\"\n",
    "\n",
    "        return self._collect_feedback(query, state, logs)\n",
    "\n",
    "    def _handle_human_escalation_path(self, query, state, logs):\n",
    "\n",
    "        # Module 5 - Sentiment\n",
    "        self.sequence_of_modules.append(5)\n",
    "        module5 = Module5_SentimentAnalysis()\n",
    "        start_time = time.time()\n",
    "        sentiment = module5.sentiment_analysis(query)\n",
    "        logs.append(self._log_execution(5, start_time, sentiment))\n",
    "        state['sentiment'] = sentiment\n",
    "\n",
    "        # Module 4 - Human Escalation\n",
    "        self.sequence_of_modules.append(4)\n",
    "        module4 = Module4_HumanEscalation()\n",
    "        start_time = time.time()\n",
    "        issue_data = module4.run(self.chat_history, sentiment)\n",
    "        logs.append(self._log_execution(4, start_time, issue_data))\n",
    "\n",
    "        # Module 7 - Knowledge Base\n",
    "        self.sequence_of_modules.append(7)\n",
    "        insurance_type = issue_data.get('insurance_type', 0)\n",
    "        module7 = Module7_KnowledgeBaseBuilder(self.chat_history, insurance_type)\n",
    "        start_time = time.time()\n",
    "        kb_update = module7.generate_draft_faq()\n",
    "        logs.append(self._log_execution(7, start_time, kb_update))\n",
    "\n",
    "        # if issue_data[\"human_attendant_name\"] = \"\", the customer was added to the waiting list\n",
    "        if issue_data[\"human_attendant_name\"] == \"\":\n",
    "            send_message_to_the_user(\"There are currently no agents available. You will be added to the waiting queue.\")\n",
    "            self.chat_history += \"\\n\\033[36mSystem\\033[0m: There are currently no agents available. You will be added to the waiting queue.\"\n",
    "            return state\n",
    "\n",
    "        human_attendant_name = issue_data[\"human_attendant_name\"]\n",
    "        human_attendant_id = issue_data[\"human_attendant_id\"]\n",
    "        state['human_attendant_name'] = human_attendant_name\n",
    "        state['human_attendant_id'] = human_attendant_id\n",
    "\n",
    "        send_message_to_the_user(f\"You are being forwarded to the agent {human_attendant_name}. Please wait a moment.\")\n",
    "\n",
    "        send_message_to_the_user(issue_data[\"recommended_message\"])\n",
    "        self.chat_history += f\"\\n\\033[34mHuman Module ({human_attendant_name})\\033[0m: {issue_data['recommended_message']}\"\n",
    "\n",
    "        print(\"\\n\\033[33mSystem\\033[0m: From here, the customer will interact with the human agent.\\n\")\n",
    "\n",
    "        ### Human agent interacts with the customer ###\n",
    "\n",
    "        # After interaction is done, make the human agent available to help other customers\n",
    "        module4.free_human_agent(human_attendant_id)\n",
    "\n",
    "        sentiment_analysis = issue_data.get('sentiment_analysis', {})\n",
    "        issue_summary = issue_data.get('issue_summary', \"\")\n",
    "        query_category = issue_data.get('query_category', \"\")\n",
    "        query_subcategory = issue_data.get('query_subcategory', \"\")\n",
    "\n",
    "        return self._collect_feedback(query, state, logs, sentiment_analysis, insurance_type, issue_summary, query_category, query_subcategory, human_attendant_name)\n",
    "\n",
    "    def _handle_compliance_failure_faq_path(self, query, state, logs):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            rag_response:str: The response from the RAG module. Or an empty string\n",
    "              if the RAG module fails to generate a compliant response.\n",
    "        \"\"\"\n",
    "        # Try once again to generate the answer\n",
    "\n",
    "        # Module 2 - RAG\n",
    "        self.sequence_of_modules.append(2)\n",
    "        module2 = Module2_RAG()\n",
    "        start_time = time.time()\n",
    "        rag_response:str = module2.process_question(query)\n",
    "        logs.append(self._log_execution(2, start_time, rag_response))\n",
    "        state['rag_response_after_compliance_retry'] = rag_response\n",
    "\n",
    "        # Check if the new response is compliant\n",
    "\n",
    "        # Module 9 - Compliance\n",
    "        self.sequence_of_modules.append(9)\n",
    "        module9 = Module9_ComplianceVerifier(query, rag_response)\n",
    "        start_time = time.time()\n",
    "        compliance_check:dict[bool,str] = module9.run_verification()\n",
    "        logs.append(self._log_execution(9, start_time, compliance_check))\n",
    "\n",
    "        if not compliance_check['compliance']:\n",
    "            return \"\"\n",
    "\n",
    "        return rag_response\n",
    "\n",
    "    def _handle_compliance_failure_contact_info_path(self, query, state, logs):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            contact_response:str: The response from the contact info module. Or an empty string\n",
    "              if the contact info module fails to generate a compliant response.\n",
    "        \"\"\"\n",
    "        # Try once again to generate the answer\n",
    "\n",
    "        # Module 3 - Contact Info\n",
    "        self.sequence_of_modules.append(3)\n",
    "        module3 = Module3_ContactInfo()\n",
    "        start_time = time.time()\n",
    "        contact_response, chat_history_module3 = module3.ask_module(query)\n",
    "        logs.append(self._log_execution(3, start_time, contact_response))\n",
    "        state['contact_response_after_compliance_retry'] = contact_response\n",
    "\n",
    "        # Check if the new response is compliant\n",
    "\n",
    "        # Module 9 - Compliance\n",
    "        self.sequence_of_modules.append(9)\n",
    "        module9 = Module9_ComplianceVerifier(query, contact_response)\n",
    "        start_time = time.time()\n",
    "        compliance_check:dict[bool,str] = module9.run_verification()\n",
    "        logs.append(self._log_execution(9, start_time, compliance_check))\n",
    "\n",
    "        if not compliance_check['compliance']:\n",
    "            return \"\"\n",
    "\n",
    "        return contact_response\n",
    "\n",
    "\n",
    "    def _collect_feedback(self, query, state, logs, sentiment_analysis:dict={}, insurance_type:int=0, issue_summary:str=\"\", query_category:str=\"\", query_subcategory:str=\"\", human_attendant_name:str=\"\"):\n",
    "        # Module 6 - Feedback\n",
    "        self.sequence_of_modules.append(6)\n",
    "        module6 = Module6_FeedbackCollector(self.chat_history, human_attendant_name, sentiment_analysis, insurance_type, issue_summary, query_category, query_subcategory)\n",
    "        start_time = time.time()\n",
    "        feedback = module6.get_customer_feedback()\n",
    "        logs.append(self._log_execution(6, start_time, feedback[\"general\"]))\n",
    "        state['general_feedback'] = feedback[\"general\"]\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXJ2luztH_za"
   },
   "source": [
    "### 3.3.2 Workflow Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17479,
     "status": "ok",
     "timestamp": 1745023366555,
     "user": {
      "displayName": "Matheus Ferracciú Scatolin",
      "userId": "06121676812765815249"
     },
     "user_tz": 180
    },
    "id": "Dgkag4WXtHK3",
    "outputId": "ea24ac8d-8b34-4ca9-fa27-01f25c8a1803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for your question! We're looking for an answer to it in the FAQ database.\n",
      "\u001b[34mAnswer: The Bradesco Dental plan works by guaranteeing access to basic dental treatments, such as cleaning and treatment of cavities, with a wide national coverage of more than 27 thousand accredited dentists. It also covers procedures like extraction, root canal, and surgeries performed in the office. You can check the complete coverage of the plan on their portal.\u001b[0m\n",
      "\u001b[32mSources (FAQ ids): [41, 42, 40, 36, 38]\u001b[0m\n",
      "I hope I helped, José! Were you able to understand how the Bradesco dental insurance works? (Y/N): \n",
      "Y\n",
      "I'm glad your issue has been resolved! Now, could you please provide us with a quick review to improve our service?\n",
      "How satisfied are you with the resolution of your problem? (1-5): 3\n",
      "We are sorry to hear that. Could you please tell us what could have been better? \n",
      "How would you rate your experience with the human agent? (1-5): 4\n",
      "How easy was it to interact with the system? (1-5): 5\n",
      "How would you rate your overall experience? (1-5): 6\n",
      "Please, enter a number between 1 and 5.\n",
      "How would you rate your overall experience? (1-5): 5\n",
      "Would you like to provide more general comments about the service? 4\n"
     ]
    }
   ],
   "source": [
    "query = \"I'm José and I would like to know how Bradesco dental insurance works.\" # 0\n",
    "# query = \"I need my insurance's dental cleaning near me. Where can I find a dentist?\" # 0 (1)\n",
    "# query = \"Where can I call to renew my health insurance?\" # 1\n",
    "# query = \"I'm Gabriel, customer from the life insurance plan. I need the help of an attendant urgently.\" # 2\n",
    "# query = \"What is the largest animal in the world?\" # 3\n",
    "\n",
    "workflow = WorkflowManager()\n",
    "final_state_and_logs = workflow.process_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V1aIVEEITlq"
   },
   "source": [
    "### 3.3.3 Log example (chat history, sequence of modules, latency of each module, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1745023369051,
     "user": {
      "displayName": "Matheus Ferracciú Scatolin",
      "userId": "06121676812765815249"
     },
     "user_tz": 180
    },
    "id": "YaMCOXs2C9V7",
    "outputId": "b71d5809-a628-42af-85ab-b67b0219a56e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chat_history\": \"\\n\\u001b[32mUser\\u001b[0m: I'm José and I would like to know how Bradesco dental insurance works.\\n\\u001b[36mSystem\\u001b[0m: We're looking for an answer to it in the FAQ database.\\n\\u001b[34mModule 2\\u001b[0m: \\u001b[34mAnswer: The Bradesco Dental plan works by guaranteeing access to basic dental treatments, such as cleaning and treatment of cavities, with a wide national coverage of more than 27 thousand accredited dentists. It also covers procedures like extraction, root canal, and surgeries performed in the office. You can check the complete coverage of the plan on their portal.\\u001b[0m\\n\\u001b[32mSources (FAQ ids): [41, 42, 40, 36, 38]\\u001b[0m\\n\\u001b[34mModule 8\\u001b[0m: I hope I helped, José! Were you able to understand how the Bradesco dental insurance works?\\n\\u001b[32mUser\\u001b[0m: Y\\n\\u001b[36mSystem\\u001b[0m: I'm glad your issue has been resolved! Now, could you please provide us with a quick review to improve our service?\",\n",
      "    \"sequence_of_modules\": [\n",
      "        1,\n",
      "        2,\n",
      "        9,\n",
      "        8,\n",
      "        6\n",
      "    ],\n",
      "    \"final_state\": {\n",
      "        \"classification\": 0,\n",
      "        \"rag_response\": \"\\u001b[34mAnswer: The Bradesco Dental plan works by guaranteeing access to basic dental treatments, such as cleaning and treatment of cavities, with a wide national coverage of more than 27 thousand accredited dentists. It also covers procedures like extraction, root canal, and surgeries performed in the office. You can check the complete coverage of the plan on their portal.\\u001b[0m\\n\\u001b[32mSources (FAQ ids): [41, 42, 40, 36, 38]\\u001b[0m\",\n",
      "        \"resolution_check_faq_path\": true,\n",
      "        \"general_feedback\": {\n",
      "            \"rating\": 4,\n",
      "            \"follow_up_response\": \"4\",\n",
      "            \"categories\": []\n",
      "        }\n",
      "    },\n",
      "    \"execution_logs\": [\n",
      "        {\n",
      "            \"module\": 1,\n",
      "            \"execution_time\": 0.6905415058135986,\n",
      "            \"timestamp\": \"2025-04-19T00:42:25.718044\",\n",
      "            \"response\": 0\n",
      "        },\n",
      "        {\n",
      "            \"module\": 2,\n",
      "            \"execution_time\": 3.2479515075683594,\n",
      "            \"timestamp\": \"2025-04-19T00:42:28.966240\",\n",
      "            \"response\": \"\\u001b[34mAnswer: The Bradesco Dental plan works by guaranteeing access to basic dental treatments, such as cleaning and treatment of cavities, with a wide national coverage of more than 27 thousand accredited dentists. It also covers procedures like extraction, root canal, and surgeries performed in the office. You can check the complete coverage of the plan on their portal.\\u001b[0m\\n\\u001b[32mSources (FAQ ids): [41, 42, 40, 36, 38]\\u001b[0m\"\n",
      "        },\n",
      "        {\n",
      "            \"module\": 9,\n",
      "            \"execution_time\": 0.1496264934539795,\n",
      "            \"timestamp\": \"2025-04-19T00:42:29.119080\",\n",
      "            \"response\": {\n",
      "                \"compliance\": true,\n",
      "                \"violation\": \"No violation\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"module\": 8,\n",
      "            \"execution_time\": 0.3693246841430664,\n",
      "            \"timestamp\": \"2025-04-19T00:42:29.491346\",\n",
      "            \"response\": \"I hope I helped, José! Were you able to understand how the Bradesco dental insurance works?\"\n",
      "        },\n",
      "        {\n",
      "            \"module\": 6,\n",
      "            \"execution_time\": 8.765531539916992,\n",
      "            \"timestamp\": \"2025-04-19T00:42:41.983407\",\n",
      "            \"response\": {\n",
      "                \"rating\": 4,\n",
      "                \"follow_up_response\": \"4\",\n",
      "                \"categories\": []\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(final_state_and_logs, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1N7WpKt_wXh"
   },
   "source": [
    "## 3.2 HUMAN AGENT (Service list)\n",
    "\n",
    "Choose the correct human agent based on the query category and insurance type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMEQcf_fM7o3"
   },
   "source": [
    "### 3.2.1 Human_agent class - for the human agent to interact with the customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZuXqKM5BoBx"
   },
   "outputs": [],
   "source": [
    "insurance_types = {\n",
    "    0: \"Standart\",\n",
    "    1: \"Auto\",\n",
    "    2: \"Health\",\n",
    "    3: \"Life\",\n",
    "    4: \"Dental\",\n",
    "    5: \"Social Security\",\n",
    "    6: \"Residential\",\n",
    "}\n",
    "category_mapping = [\"Policy Management\", \"Claims\", \"Payments\", \"General Questions\", \"Technical Problems\", \"Human Support Escalations\", \"Regulatory or Compliance Questions\"]\n",
    "\n",
    "class Human_agent:\n",
    "    def __init__(self, human_attendant_name, insurance_type:int=0, query_category:str=\"General Questions\",\n",
    "                 status:str=\"Available\",\n",
    "                 waiting_list_path:str=f\"{STELLAR_path}/outputs/module_4/waiting_list.json\"):\n",
    "        self.human_attendant_name = human_attendant_name\n",
    "        self.insurance_type = insurance_type\n",
    "        self.status = status\n",
    "        self.query_category = query_category\n",
    "        self.waiting_list_path = waiting_list_path\n",
    "\n",
    "    def find_customer(self):\n",
    "        \"\"\"\n",
    "        This is the function a human agent can use to find a customer in the waiting list.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.waiting_list_path, \"r\") as f:\n",
    "                waiting_list = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            waiting_list = []\n",
    "\n",
    "        if len(waiting_list) == 0:\n",
    "            print(\"There are no appointments in the queue.\")\n",
    "            return None\n",
    "\n",
    "        # if there is a customer that is a perfect match for this agent, serve this customer\n",
    "        for i in range(len(waiting_list)):\n",
    "            if waiting_list[i][\"insurance_type\"] == self.insurance_type and waiting_list[i][\"query_category\"] == self.query_category:\n",
    "                customer = waiting_list.pop(i)\n",
    "                with open(self.waiting_list_path, \"w\") as f:\n",
    "                    json.dump(waiting_list, f, indent=2, ensure_ascii=False)\n",
    "                self.customer_service(customer)\n",
    "                return\n",
    "\n",
    "        # Check for partial matches\n",
    "        for i in range(len(waiting_list)):\n",
    "          if waiting_list[i][\"insurance_type\"] == self.insurance_type:\n",
    "              customer = waiting_list.pop(i)\n",
    "              with open(self.waiting_list_path, \"w\") as f:\n",
    "                  json.dump(waiting_list, f, indent=2, ensure_ascii=False)\n",
    "              self.customer_service(customer)\n",
    "              return\n",
    "\n",
    "          if waiting_list[i][\"query_category\"] == self.query_category:\n",
    "              customer = waiting_list.pop(i)\n",
    "              with open(self.waiting_list_path, \"w\") as f:\n",
    "                  json.dump(waiting_list, f, indent=2, ensure_ascii=False)\n",
    "              self.customer_service(customer)\n",
    "              return\n",
    "\n",
    "        # if there is no partial match, serve the customer with the highest urgency in the waiting list\n",
    "        waiting_list.sort(key=lambda x: x[\"urgency_score\"], reverse=True)\n",
    "        customer = waiting_list.pop(0)\n",
    "        with open(self.waiting_list_path, \"w\") as f:\n",
    "            json.dump(waiting_list, f, indent=2, ensure_ascii=False)\n",
    "        self.customer_service(customer)\n",
    "        return\n",
    "\n",
    "    def customer_service(self, issue_data:dict):\n",
    "        \"\"\"\n",
    "        This is how the human agent interacts with the customer. First, he receives the issue data,\n",
    "        and then he can start the interaction with the customer.\n",
    "        Args:\n",
    "            issue_data (dict): a dict with the issue data in this format:\n",
    "            {\"sentiment\": <dict>, \"chat_history\": <str>, \"human_attendant_name\": <str>, \"model\": <str>,\n",
    "            \"customer_name\": <str>, \"insurance_type\": <int>, \"issue_summary\": <str>, \"query_category\": <str>,\n",
    "            \"query_subcategory\": <str>, \"urgency_score\": <int>, \"recommended_message\": <str>}\n",
    "        \"\"\"\n",
    "        print(f\"\\033[34mHuman Module ({issue_data['human_attendant_name']})\\033[0m: {issue_data['recommended_message']}\")\n",
    "\n",
    "        # From now on, the human agent should take control of the interaction\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1745023383872,
     "user": {
      "displayName": "Matheus Ferracciú Scatolin",
      "userId": "06121676812765815249"
     },
     "user_tz": 180
    },
    "id": "Z7dneuF6X2vw",
    "outputId": "b0be439c-2f9a-40b7-aee1-f8599496aeec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mHuman Module (Letícia)\u001b[0m: Hello Doug, this is Letícia, your attendant from the health insurance area of Bradesco Seguros. I understand you need to discuss something related to your health insurance, and I'm here to listen and help in any way I can. Your concerns are important to me, and I want to make sure you receive the support you need. What would you like to talk about regarding your health insurance today?\n"
     ]
    }
   ],
   "source": [
    "human_agent = Human_agent(\"Marcos\", insurance_type=0, query_category=\"General Questions\")\n",
    "human_agent.find_customer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32tvO132M6oB"
   },
   "source": [
    "### 3.2.2 Creating some examples of human agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qa_iYO-JNNqf"
   },
   "outputs": [],
   "source": [
    "human_agents = []\n",
    "human_agents.append(Human_agent(\"Marcos\", insurance_type=0, query_category=\"General Questions\"))\n",
    "human_agents.append(Human_agent(\"Maria\", insurance_type=1, query_category=\"Claims\"))\n",
    "human_agents.append(Human_agent(\"João\", insurance_type=2, query_category=\"Policy Management\"))\n",
    "human_agents.append(Human_agent(\"Ana\", insurance_type=3, query_category=\"Payments\"))\n",
    "human_agents.append(Human_agent(\"Pedro\", insurance_type=4, query_category=\"General Questions\"))\n",
    "human_agents.append(Human_agent(\"Carla\", insurance_type=5, query_category=\"Human Support Escalations\"))\n",
    "human_agents.append(Human_agent(\"Lucas\", insurance_type=6, query_category=\"Regulatory or Compliance Questions\"))\n",
    "human_agents.append(Human_agent(\"Fernanda\", insurance_type=0, query_category=\"Claims\"))\n",
    "human_agents.append(Human_agent(\"Rafael\", insurance_type=1, query_category=\"Policy Management\"))\n",
    "human_agents.append(Human_agent(\"Isabela\", insurance_type=2, query_category=\"Payments\"))\n",
    "human_agents.append(Human_agent(\"Gustavo\", insurance_type=3, query_category=\"General Questions\"))\n",
    "human_agents.append(Human_agent(\"Juliana\", insurance_type=4, query_category=\"Human Support Escalations\"))\n",
    "human_agents.append(Human_agent(\"Diego\", insurance_type=5, query_category=\"Regulatory or Compliance Questions\"))\n",
    "human_agents.append(Human_agent(\"Larissa\", insurance_type=6, query_category=\"Claims\"))\n",
    "human_agents.append(Human_agent(\"Bruno\", insurance_type=0, query_category=\"Policy Management\"))\n",
    "human_agents.append(Human_agent(\"Amanda\", insurance_type=1, query_category=\"Payments\"))\n",
    "human_agents.append(Human_agent(\"Ricardo\", insurance_type=2, query_category=\"General Questions\"))\n",
    "human_agents.append(Human_agent(\"Camila\", insurance_type=3, query_category=\"Human Support Escalations\"))\n",
    "human_agents.append(Human_agent(\"Gabriel\", insurance_type=4, query_category=\"Regulatory or Compliance Questions\"))\n",
    "human_agents.append(Human_agent(\"Renata\", insurance_type=5, query_category=\"Claims\"))\n",
    "human_agents.append(Human_agent(\"Felipe\", insurance_type=6, query_category=\"Policy Management\"))\n",
    "human_agents.append(Human_agent(\"Lívia\", insurance_type=0, query_category=\"Payments\"))\n",
    "human_agents.append(Human_agent(\"Matheus\", insurance_type=1, query_category=\"General Questions\"))\n",
    "human_agents.append(Human_agent(\"Letícia\", insurance_type=2, query_category=\"Human Support Escalations\"))\n",
    "human_agents.append(Human_agent(\"Vinicius\", insurance_type=3, query_category=\"Regulatory or Compliance Questions\"))\n",
    "\n",
    "module4 = Module4_HumanEscalation()\n",
    "# To add the new agents, remove the comments bellow:\n",
    "#for human_agent in human_agents:\n",
    "#    module4.add_human_agent(human_agent)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "QAy-DGlJXirS",
    "RDaSsXz0YPfO",
    "JOIX25GqZTCB",
    "ytSguSoejHRN",
    "FMEf2c9iogFN",
    "5AHa-S-hG7tl",
    "X8z8ygPWMQvx",
    "Suddcf6bRY9z",
    "DOsKCQFazH5q",
    "aJCjG7U70iAZ",
    "_7ogq68YH4R-",
    "cXJ2luztH_za",
    "5V1aIVEEITlq",
    "32tvO132M6oB"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
